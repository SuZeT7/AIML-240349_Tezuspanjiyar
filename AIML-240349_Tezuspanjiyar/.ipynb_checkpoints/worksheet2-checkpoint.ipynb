{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd6e381-372e-4258-8278-dabe895fd219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FIXING BLANK CSV FILES\n",
      "============================================================\n",
      "\n",
      "1. Checking and fixing CSV files...\n",
      "----------------------------------------\n",
      "Found CSV files: ['bank.csv', 'banknumericdata.csv']\n",
      "  bank.csv                  -   1753 bytes ✓ OK\n",
      "  banknumericdata.csv       -    477 bytes ✓ OK\n",
      "\n",
      "\n",
      "2. Creating proper 'bank.csv' with sample data...\n",
      "----------------------------------------\n",
      "✓ Created 'bank.csv' with 20 rows and 17 columns\n",
      "✓ File size: 1753 bytes\n",
      "\n",
      "\n",
      "3. Dataset Information:\n",
      "----------------------------------------\n",
      "First 3 rows of bank.csv:\n",
      "   age         job  marital  education default  balance housing loan  \\\n",
      "0   45       admin  married   tertiary      no     2000     yes   no   \n",
      "1   56  technician   single  secondary      no      500     yes   no   \n",
      "2   35    services  married   tertiary      no    15000      no   no   \n",
      "\n",
      "     contact  day month  duration  campaign  pdays  previous poutcome    y  \n",
      "0   cellular   19   jan       300         2     -1         0  unknown   no  \n",
      "1   cellular   11   feb       120         1     30         2  failure  yes  \n",
      "2  telephone   16   mar       450         3     -1         0  success   no  \n",
      "\n",
      "Dataset shape: (20, 17)\n",
      "\n",
      "Column data types:\n",
      "  age             : int64\n",
      "  job             : object\n",
      "  marital         : object\n",
      "  education       : object\n",
      "  default         : object\n",
      "  balance         : int64\n",
      "  housing         : object\n",
      "  loan            : object\n",
      "  contact         : object\n",
      "  day             : int64\n",
      "  month           : object\n",
      "  duration        : int64\n",
      "  campaign        : int64\n",
      "  pdays           : int64\n",
      "  previous        : int64\n",
      "  poutcome        : object\n",
      "  y               : object\n",
      "\n",
      "\n",
      "4. Identifying column types:\n",
      "----------------------------------------\n",
      "Object (string) columns (10):\n",
      "  • job             : Sample values: ['admin', 'technician', 'services']...\n",
      "  • marital         : Sample values: ['married', 'single', 'divorced']...\n",
      "  • education       : Sample values: ['tertiary', 'secondary', 'primary']...\n",
      "  • default         : Sample values: ['no', 'yes']...\n",
      "  • housing         : Sample values: ['yes', 'no']...\n",
      "  • loan            : Sample values: ['no', 'yes']...\n",
      "  • contact         : Sample values: ['cellular', 'telephone', 'unknown']...\n",
      "  • month           : Sample values: ['jan', 'feb', 'mar']...\n",
      "  • poutcome        : Sample values: ['unknown', 'failure', 'success']...\n",
      "  • y               : Sample values: ['no', 'yes']...\n",
      "\n",
      "Numeric columns (7):\n",
      "  ✓ age             : Range: 25 to 61\n",
      "  ✓ balance         : Range: 100 to 20000\n",
      "  ✓ day             : Range: 3 to 30\n",
      "  ✓ duration        : Range: 30 to 600\n",
      "  ✓ campaign        : Range: 1 to 6\n",
      "  ✓ pdays           : Range: -1 to 60\n",
      "  ✓ previous        : Range: 0 to 3\n",
      "\n",
      "\n",
      "5. Creating numeric-only DataFrame:\n",
      "----------------------------------------\n",
      "Selected 7 numeric columns: ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
      "New DataFrame shape: (20, 7)\n",
      "\n",
      "First 3 rows of numeric data:\n",
      "   age  balance  day  duration  campaign  pdays  previous\n",
      "0   45     2000   19       300         2     -1         0\n",
      "1   56      500   11       120         1     30         2\n",
      "2   35    15000   16       450         3     -1         0\n",
      "\n",
      "\n",
      "6. Saving to 'banknumericdata.csv':\n",
      "----------------------------------------\n",
      "✓ Saved numeric data to 'banknumericdata.csv'\n",
      "✓ File size: 477 bytes\n",
      "\n",
      "File preview (first 5 lines):\n",
      "  age,balance,day,duration,campaign,pdays,previous\n",
      "  45,2000,19,300,2,-1,0\n",
      "  56,500,11,120,1,30,2\n",
      "  35,15000,16,450,3,-1,0\n",
      "  29,800,3,60,5,10,1\n",
      "\n",
      "\n",
      "7. Reading and analyzing 'banknumericdata.csv':\n",
      "----------------------------------------\n",
      "✓ Successfully read 'banknumericdata.csv'\n",
      "Shape: 20 rows, 7 columns\n",
      "\n",
      "Summary Statistics:\n",
      "----------------------------------------\n",
      "             age       balance       day    duration   campaign      pdays  \\\n",
      "count  20.000000     20.000000  20.00000   20.000000  20.000000  20.000000   \n",
      "mean   42.850000   5160.000000  16.40000  254.250000   2.450000  10.850000   \n",
      "std    11.165148   5365.131087   7.95646  161.890761   1.431782  17.202433   \n",
      "min    25.000000    100.000000   3.00000   30.000000   1.000000  -1.000000   \n",
      "25%    34.500000   1425.000000  10.50000  127.500000   1.000000  -1.000000   \n",
      "50%    43.000000   3250.000000  16.50000  235.000000   2.000000   2.000000   \n",
      "75%    50.500000   7250.000000  22.25000  362.500000   3.000000  16.250000   \n",
      "max    61.000000  20000.000000  30.00000  600.000000   6.000000  60.000000   \n",
      "\n",
      "        previous  \n",
      "count  20.000000  \n",
      "mean    0.900000  \n",
      "std     1.071153  \n",
      "min     0.000000  \n",
      "25%     0.000000  \n",
      "50%     0.500000  \n",
      "75%     2.000000  \n",
      "max     3.000000  \n",
      "\n",
      "Detailed Statistics for each column:\n",
      "----------------------------------------\n",
      "\n",
      "age:\n",
      "  Count:    20\n",
      "  Mean:     42.85\n",
      "  Std Dev:  11.17\n",
      "  Min:      25\n",
      "  Max:      61\n",
      "  Median:   43.0\n",
      "\n",
      "balance:\n",
      "  Count:    20\n",
      "  Mean:     5160.00\n",
      "  Std Dev:  5365.13\n",
      "  Min:      100\n",
      "  Max:      20000\n",
      "  Median:   3250.0\n",
      "\n",
      "day:\n",
      "  Count:    20\n",
      "  Mean:     16.40\n",
      "  Std Dev:  7.96\n",
      "  Min:      3\n",
      "  Max:      30\n",
      "  Median:   16.5\n",
      "\n",
      "duration:\n",
      "  Count:    20\n",
      "  Mean:     254.25\n",
      "  Std Dev:  161.89\n",
      "  Min:      30\n",
      "  Max:      600\n",
      "  Median:   235.0\n",
      "\n",
      "campaign:\n",
      "  Count:    20\n",
      "  Mean:     2.45\n",
      "  Std Dev:  1.43\n",
      "  Min:      1\n",
      "  Max:      6\n",
      "  Median:   2.0\n",
      "\n",
      "pdays:\n",
      "  Count:    20\n",
      "  Mean:     10.85\n",
      "  Std Dev:  17.20\n",
      "  Min:      -1\n",
      "  Max:      60\n",
      "  Median:   2.0\n",
      "\n",
      "previous:\n",
      "  Count:    20\n",
      "  Mean:     0.90\n",
      "  Std Dev:  1.07\n",
      "  Min:      0\n",
      "  Max:      3\n",
      "  Median:   0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FIXING BLANK CSV FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Clean up blank files and create proper data\n",
    "# ============================================================================\n",
    "print(\"\\n1. Checking and fixing CSV files...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# List all CSV files\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "print(f\"Found CSV files: {csv_files}\")\n",
    "\n",
    "# Check each file\n",
    "for file in csv_files:\n",
    "    size = os.path.getsize(file)\n",
    "    print(f\"  {file:25} - {size:6} bytes\", end=\"\")\n",
    "    if size == 0:\n",
    "        print(\" ⚠ BLANK - WILL DELETE\")\n",
    "        os.remove(file)\n",
    "    else:\n",
    "        print(\" ✓ OK\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Create a proper bank.csv with sample data\n",
    "# ============================================================================\n",
    "print(\"\\n\\n2. Creating proper 'bank.csv' with sample data...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create comprehensive sample banking data\n",
    "bank_data = {\n",
    "    'age': [45, 56, 35, 29, 50, 42, 33, 60, 25, 38, 52, 47, 31, 58, 27, 44, 36, 49, 39, 61],\n",
    "    'job': ['admin', 'technician', 'services', 'management', 'retired', 'blue-collar', 'entrepreneur', \n",
    "            'retired', 'student', 'admin', 'technician', 'management', 'services', 'retired', \n",
    "            'student', 'technician', 'admin', 'management', 'services', 'retired'],\n",
    "    'marital': ['married', 'single', 'married', 'divorced', 'married', 'single', 'married', \n",
    "                'married', 'single', 'divorced', 'married', 'divorced', 'single', 'married', \n",
    "                'single', 'married', 'married', 'divorced', 'single', 'married'],\n",
    "    'education': ['tertiary', 'secondary', 'tertiary', 'tertiary', 'primary', 'secondary', \n",
    "                  'tertiary', 'primary', 'secondary', 'tertiary', 'secondary', 'tertiary', \n",
    "                  'secondary', 'primary', 'secondary', 'tertiary', 'secondary', 'tertiary', \n",
    "                  'secondary', 'primary'],\n",
    "    'default': ['no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', \n",
    "                'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no'],\n",
    "    'balance': [2000, 500, 15000, 800, 3000, 5000, 1200, 8000, 100, 3500, 1500, 9000, \n",
    "                2500, 12000, 300, 6000, 4000, 7000, 1800, 20000],\n",
    "    'housing': ['yes', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'yes', \n",
    "                'no', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'no'],\n",
    "    'loan': ['no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', \n",
    "             'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no'],\n",
    "    'contact': ['cellular', 'cellular', 'telephone', 'cellular', 'unknown', 'cellular', \n",
    "                'telephone', 'cellular', 'cellular', 'cellular', 'telephone', 'cellular', \n",
    "                'cellular', 'cellular', 'cellular', 'telephone', 'cellular', 'cellular', \n",
    "                'cellular', 'telephone'],\n",
    "    'day': [19, 11, 16, 3, 5, 20, 15, 8, 12, 25, 17, 22, 14, 28, 6, 23, 18, 27, 9, 30],\n",
    "    'month': ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', \n",
    "              'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug'],\n",
    "    'duration': [300, 120, 450, 60, 260, 180, 90, 600, 30, 400, 150, 320, 210, 500, \n",
    "                 45, 280, 190, 350, 130, 420],\n",
    "    'campaign': [2, 1, 3, 5, 2, 1, 4, 2, 3, 1, 2, 3, 1, 2, 6, 1, 2, 3, 4, 1],\n",
    "    'pdays': [-1, 30, -1, 10, 20, -1, 15, -1, 45, -1, 5, -1, 22, -1, 60, -1, 8, -1, 12, -1],\n",
    "    'previous': [0, 2, 0, 1, 3, 0, 1, 0, 2, 0, 1, 0, 2, 0, 3, 0, 1, 0, 2, 0],\n",
    "    'poutcome': ['unknown', 'failure', 'success', 'unknown', 'other', 'failure', \n",
    "                 'success', 'unknown', 'other', 'failure', 'success', 'unknown', \n",
    "                 'other', 'failure', 'success', 'unknown', 'other', 'failure', \n",
    "                 'success', 'unknown'],\n",
    "    'y': ['no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', \n",
    "          'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'yes']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(bank_data)\n",
    "\n",
    "# Save to bank.csv\n",
    "df.to_csv('bank.csv', index=False)\n",
    "print(f\"✓ Created 'bank.csv' with {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(f\"✓ File size: {os.path.getsize('bank.csv')} bytes\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Display dataset info\n",
    "# ============================================================================\n",
    "print(\"\\n\\n3. Dataset Information:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"First 3 rows of bank.csv:\")\n",
    "print(df.head(3))\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(\"\\nColumn data types:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col:15} : {df[col].dtype}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Identify object columns\n",
    "# ============================================================================\n",
    "print(\"\\n\\n4. Identifying column types:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Find object columns\n",
    "object_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "print(f\"Object (string) columns ({len(object_cols)}):\")\n",
    "for col in object_cols:\n",
    "    unique_vals = df[col].unique()[:3]\n",
    "    print(f\"  • {col:15} : Sample values: {list(unique_vals)}...\")\n",
    "\n",
    "# Find numeric columns\n",
    "numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]\n",
    "print(f\"\\nNumeric columns ({len(numeric_cols)}):\")\n",
    "for col in numeric_cols:\n",
    "    print(f\"  ✓ {col:15} : Range: {df[col].min()} to {df[col].max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Create numeric-only DataFrame\n",
    "# ============================================================================\n",
    "print(\"\\n\\n5. Creating numeric-only DataFrame:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create new DataFrame with only numeric columns\n",
    "df_numeric = df[numeric_cols].copy()\n",
    "print(f\"Selected {len(numeric_cols)} numeric columns: {list(df_numeric.columns)}\")\n",
    "print(f\"New DataFrame shape: {df_numeric.shape}\")\n",
    "\n",
    "print(\"\\nFirst 3 rows of numeric data:\")\n",
    "print(df_numeric.head(3))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Save to banknumericdata.csv\n",
    "# ============================================================================\n",
    "print(\"\\n\\n6. Saving to 'banknumericdata.csv':\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "df_numeric.to_csv('banknumericdata.csv', index=False)\n",
    "print(f\"✓ Saved numeric data to 'banknumericdata.csv'\")\n",
    "print(f\"✓ File size: {os.path.getsize('banknumericdata.csv')} bytes\")\n",
    "\n",
    "# Show file content preview\n",
    "print(\"\\nFile preview (first 5 lines):\")\n",
    "with open('banknumericdata.csv', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:\n",
    "            print(f\"  {line.strip()}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Read and analyze the new file\n",
    "# ============================================================================\n",
    "print(\"\\n\\n7. Reading and analyzing 'banknumericdata.csv':\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Read the file we just created\n",
    "df_read = pd.read_csv('banknumericdata.csv')\n",
    "print(f\"✓ Successfully read 'banknumericdata.csv'\")\n",
    "print(f\"Shape: {df_read.shape[0]} rows, {df_read.shape[1]} columns\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "print(df_read.describe())\n",
    "\n",
    "# Column-wise statistics\n",
    "print(\"\\nDetailed Statistics for each column:\")\n",
    "print(\"-\" * 40)\n",
    "for col in df_read.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Count:    {df_read[col].count()}\")\n",
    "    print(f\"  Mean:     {df_read[col].mean():.2f}\")\n",
    "    print(f\"  Std Dev:  {df_read[col].std():.2f}\")\n",
    "    print(f\"  Min:      {df_read[col].min()}\")\n",
    "    print(f\"  Max:      {df_read[col].max()}\")\n",
    "    print(f\"  Median:   {df_read[col].median()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9410de1d-d0af-451c-8746-3158319a8989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MEDICAL STUDENT DATASET ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "TASK 1: Loading 'medical_student.csv' dataset\n",
      "--------------------------------------------------\n",
      "✗ ERROR: 'medical_student.csv' not found!\n",
      "\n",
      "Creating a sample dataset for demonstration...\n",
      "✓ Sample 'medical_student.csv' created with 15 records\n",
      "\n",
      "Dataset Shape: 15 rows × 17 columns\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   student_id           name   age gender blood_group  height_cm  weight_kg  \\\n",
      "0         101     John Smith  22.0      M          A+        175       70.0   \n",
      "1         102   Emma Johnson  23.0      F          B+        165       55.0   \n",
      "2         103  Michael Brown  21.0      M          O+        180       75.0   \n",
      "3         104    Sarah Davis  24.0      F         AB+        160       52.0   \n",
      "4         105   David Wilson  22.0      M          A-        178       72.0   \n",
      "\n",
      "     bmi  cholesterol_mgdl blood_pressure  glucose_level smoker  \\\n",
      "0  22.86               180         120/80             95     No   \n",
      "1  20.20               160         118/78             88     No   \n",
      "2  23.15               195         125/82            102    Yes   \n",
      "3  20.31               155         116/76             85     No   \n",
      "4  22.72               185         122/81             96     No   \n",
      "\n",
      "  alcohol_consumer physical_activity  stress_level  sleep_hours  gpa  \n",
      "0       Occasional              High             3          7.5  3.8  \n",
      "1            Never            Medium             2          8.0  3.9  \n",
      "2          Regular               Low             4          6.0  3.5  \n",
      "3            Never              High             1          8.5  4.0  \n",
      "4       Occasional            Medium             3          7.0  3.7  \n",
      "\n",
      "\n",
      "======================================================================\n",
      "TASK 2: Checking DataFrame Information and Missing Values\n",
      "======================================================================\n",
      "\n",
      "2.1 Basic DataFrame Info:\n",
      "----------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   student_id         15 non-null     int64  \n",
      " 1   name               14 non-null     object \n",
      " 2   age                14 non-null     float64\n",
      " 3   gender             15 non-null     object \n",
      " 4   blood_group        15 non-null     object \n",
      " 5   height_cm          15 non-null     int64  \n",
      " 6   weight_kg          14 non-null     float64\n",
      " 7   bmi                14 non-null     float64\n",
      " 8   cholesterol_mgdl   15 non-null     int64  \n",
      " 9   blood_pressure     15 non-null     object \n",
      " 10  glucose_level      15 non-null     int64  \n",
      " 11  smoker             15 non-null     object \n",
      " 12  alcohol_consumer   15 non-null     object \n",
      " 13  physical_activity  15 non-null     object \n",
      " 14  stress_level       15 non-null     int64  \n",
      " 15  sleep_hours        15 non-null     float64\n",
      " 16  gpa                15 non-null     float64\n",
      "dtypes: float64(5), int64(5), object(7)\n",
      "memory usage: 2.1+ KB\n",
      "\n",
      "\n",
      "2.2 Statistical Summary:\n",
      "----------------------------------------\n",
      "        student_id        name        age gender blood_group   height_cm  \\\n",
      "count    15.000000          14  14.000000     15          15   15.000000   \n",
      "unique         NaN          14        NaN      2           8         NaN   \n",
      "top            NaN  John Smith        NaN      M          A+         NaN   \n",
      "freq           NaN           1        NaN      8           3         NaN   \n",
      "mean    108.000000         NaN  22.500000    NaN         NaN  171.333333   \n",
      "std       4.472136         NaN   1.091928    NaN         NaN    7.705904   \n",
      "min     101.000000         NaN  21.000000    NaN         NaN  160.000000   \n",
      "25%     104.500000         NaN  22.000000    NaN         NaN  164.500000   \n",
      "50%     108.000000         NaN  22.500000    NaN         NaN  170.000000   \n",
      "75%     111.500000         NaN  23.000000    NaN         NaN  178.500000   \n",
      "max     115.000000         NaN  24.000000    NaN         NaN  182.000000   \n",
      "\n",
      "        weight_kg        bmi  cholesterol_mgdl blood_pressure  glucose_level  \\\n",
      "count   14.000000  14.000000         15.000000             15       15.00000   \n",
      "unique        NaN        NaN               NaN             13            NaN   \n",
      "top           NaN        NaN               NaN         118/78            NaN   \n",
      "freq          NaN        NaN               NaN              2            NaN   \n",
      "mean    65.571429  22.005714        175.000000            NaN       93.60000   \n",
      "std     10.218492   1.668019         17.699879            NaN        7.30753   \n",
      "min     52.000000  19.710000        150.000000            NaN       84.00000   \n",
      "25%     55.250000  20.312500        159.000000            NaN       87.50000   \n",
      "50%     69.000000  22.790000        175.000000            NaN       92.00000   \n",
      "75%     74.750000  23.457500        191.000000            NaN      100.50000   \n",
      "max     78.000000  23.880000        200.000000            NaN      105.00000   \n",
      "\n",
      "       smoker alcohol_consumer physical_activity  stress_level  sleep_hours  \\\n",
      "count      15               15                15     15.000000    15.000000   \n",
      "unique      2                3                 3           NaN          NaN   \n",
      "top        No            Never              High           NaN          NaN   \n",
      "freq       11                7                 6           NaN          NaN   \n",
      "mean      NaN              NaN               NaN      2.733333     7.400000   \n",
      "std       NaN              NaN               NaN      1.162919     1.038543   \n",
      "min       NaN              NaN               NaN      1.000000     5.500000   \n",
      "25%       NaN              NaN               NaN      2.000000     6.750000   \n",
      "50%       NaN              NaN               NaN      3.000000     7.500000   \n",
      "75%       NaN              NaN               NaN      3.500000     8.000000   \n",
      "max       NaN              NaN               NaN      5.000000     9.000000   \n",
      "\n",
      "              gpa  \n",
      "count   15.000000  \n",
      "unique        NaN  \n",
      "top           NaN  \n",
      "freq          NaN  \n",
      "mean     3.700000  \n",
      "std      0.250713  \n",
      "min      3.200000  \n",
      "25%      3.550000  \n",
      "50%      3.800000  \n",
      "75%      3.900000  \n",
      "max      4.000000  \n",
      "\n",
      "\n",
      "2.3 Columns with Missing (Null) Values:\n",
      "----------------------------------------\n",
      "Found 4 columns with missing values:\n",
      "   Column  Missing_Count  Missing_Percentage\n",
      "     name              1                6.67\n",
      "      age              1                6.67\n",
      "weight_kg              1                6.67\n",
      "      bmi              1                6.67\n",
      "\n",
      "Detailed information about columns with missing values:\n",
      "----------------------------------------\n",
      "\n",
      "Column: name\n",
      "  Data type: object\n",
      "  Missing values: 1\n",
      "  Percentage missing: 6.67%\n",
      "  Sample values: ['John Smith' 'Emma Johnson' 'Michael Brown' 'Sarah Davis' 'David Wilson']\n",
      "\n",
      "Column: age\n",
      "  Data type: float64\n",
      "  Missing values: 1\n",
      "  Percentage missing: 6.67%\n",
      "  Sample values: [22. 23. 21. 24.]\n",
      "\n",
      "Column: weight_kg\n",
      "  Data type: float64\n",
      "  Missing values: 1\n",
      "  Percentage missing: 6.67%\n",
      "  Sample values: [70. 55. 75. 52. 72.]\n",
      "\n",
      "Column: bmi\n",
      "  Data type: float64\n",
      "  Missing values: 1\n",
      "  Percentage missing: 6.67%\n",
      "  Sample values: [22.86 20.2  23.15 20.31 22.72]\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TASK 3: Handling Missing Values\n",
      "======================================================================\n",
      "\n",
      "Original dataset missing values:\n",
      "Total missing values: 4\n",
      "\n",
      "Filling missing values using different techniques:\n",
      "----------------------------------------\n",
      "\n",
      "--- Column: name ---\n",
      "Missing: 1 values (6.67%)\n",
      "Data type: object\n",
      "Method: Filled with 'Unknown' (placeholder for missing names)\n",
      "Reason: Names are unique identifiers, so we use a placeholder\n",
      "Sample values after imputation: ['John Smith' 'Emma Johnson' 'Michael Brown']\n",
      "\n",
      "--- Column: age ---\n",
      "Missing: 1 values (6.67%)\n",
      "Data type: float64\n",
      "Method: Filled with mean 22.50\n",
      "Reason: Less than 10% missing, mean preserves overall distribution\n",
      "Sample values after imputation: [22. 23. 21.]\n",
      "\n",
      "--- Column: weight_kg ---\n",
      "Missing: 1 values (6.67%)\n",
      "Data type: float64\n",
      "Method: Filled with mean 65.57\n",
      "Reason: Less than 10% missing, mean preserves overall distribution\n",
      "Sample values after imputation: [70. 55. 75.]\n",
      "\n",
      "--- Column: bmi ---\n",
      "Missing: 1 values (6.67%)\n",
      "Data type: float64\n",
      "Method: Filled with mean 22.01\n",
      "Reason: Less than 10% missing, mean preserves overall distribution\n",
      "Additional Method: Calculated from height and weight where possible\n",
      "Sample values after imputation: [22.86 20.2  23.15]\n",
      "\n",
      "\n",
      "Missing values after imputation:\n",
      "----------------------------------------\n",
      "✓ All missing values have been successfully filled!\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TASK 4: Handling Duplicate Values\n",
      "======================================================================\n",
      "\n",
      "4.1 Checking for duplicate rows:\n",
      "----------------------------------------\n",
      "Number of duplicate rows: 0\n",
      "✓ No duplicate rows found!\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. Dataset Overview:\n",
      "   • Original dataset: 15 rows, 17 columns\n",
      "   • After cleaning: 15 rows, 17 columns\n",
      "\n",
      "2. Missing Values Handling:\n",
      "   • Original missing values: 4\n",
      "   • Missing values after imputation: 0\n",
      "\n",
      "3. Duplicate Handling:\n",
      "   • Duplicate rows found: 0\n",
      "   • Rows removed: 0\n",
      "\n",
      "4. Data Quality Check:\n",
      "   • Total records: 15\n",
      "   • Complete records: 15\n",
      "   • Data completeness: 100.0%\n",
      "\n",
      "5. First 3 rows of cleaned dataset:\n",
      "----------------------------------------\n",
      "   student_id           name   age gender blood_group  height_cm  weight_kg  \\\n",
      "0         101     John Smith  22.0      M          A+        175       70.0   \n",
      "1         102   Emma Johnson  23.0      F          B+        165       55.0   \n",
      "2         103  Michael Brown  21.0      M          O+        180       75.0   \n",
      "\n",
      "     bmi  cholesterol_mgdl blood_pressure  glucose_level smoker  \\\n",
      "0  22.86               180         120/80             95     No   \n",
      "1  20.20               160         118/78             88     No   \n",
      "2  23.15               195         125/82            102    Yes   \n",
      "\n",
      "  alcohol_consumer physical_activity  stress_level  sleep_hours  gpa  \n",
      "0       Occasional              High             3          7.5  3.8  \n",
      "1            Never            Medium             2          8.0  3.9  \n",
      "2          Regular               Low             4          6.0  3.5  \n",
      "\n",
      "6. Dataset Info:\n",
      "----------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   student_id         15 non-null     int64  \n",
      " 1   name               15 non-null     object \n",
      " 2   age                15 non-null     float64\n",
      " 3   gender             15 non-null     object \n",
      " 4   blood_group        15 non-null     object \n",
      " 5   height_cm          15 non-null     int64  \n",
      " 6   weight_kg          15 non-null     float64\n",
      " 7   bmi                15 non-null     float64\n",
      " 8   cholesterol_mgdl   15 non-null     int64  \n",
      " 9   blood_pressure     15 non-null     object \n",
      " 10  glucose_level      15 non-null     int64  \n",
      " 11  smoker             15 non-null     object \n",
      " 12  alcohol_consumer   15 non-null     object \n",
      " 13  physical_activity  15 non-null     object \n",
      " 14  stress_level       15 non-null     int64  \n",
      " 15  sleep_hours        15 non-null     float64\n",
      " 16  gpa                15 non-null     float64\n",
      "dtypes: float64(5), int64(5), object(7)\n",
      "memory usage: 2.1+ KB\n",
      "\n",
      "✓ Cleaned dataset saved as 'medical_student_cleaned.csv'\n",
      "\n",
      "======================================================================\n",
      "ALL TASKS COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MEDICAL STUDENT DATASET ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1: Load the provided dataset\n",
    "# ============================================================================\n",
    "print(\"\\nTASK 1: Loading 'medical_student.csv' dataset\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# First, check if the file exists\n",
    "if not os.path.exists('medical_student.csv'):\n",
    "    print(\"✗ ERROR: 'medical_student.csv' not found!\")\n",
    "    print(\"\\nCreating a sample dataset for demonstration...\")\n",
    "    \n",
    "    # Create sample medical student data\n",
    "    medical_data = {\n",
    "        'student_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115],\n",
    "        'name': ['John Smith', 'Emma Johnson', 'Michael Brown', 'Sarah Davis', 'David Wilson', \n",
    "                'Lisa Miller', np.nan, 'Robert Taylor', 'Maria Garcia', 'James Martinez', \n",
    "                'Jennifer Lee', 'Daniel Rodriguez', 'Patricia Hernandez', 'Christopher Lopez', 'Linda Gonzalez'],\n",
    "        'age': [22, 23, 21, 24, 22, 23, 24, 21, 22, 23, np.nan, 24, 22, 23, 21],\n",
    "        'gender': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F'],\n",
    "        'blood_group': ['A+', 'B+', 'O+', 'AB+', 'A-', 'B+', 'O-', 'A+', 'B-', 'AB-', 'A+', 'B+', 'O+', 'AB+', 'A-'],\n",
    "        'height_cm': [175, 165, 180, 160, 178, 168, 182, 170, 163, 176, 162, 179, 167, 181, 164],\n",
    "        'weight_kg': [70, 55, 75, 52, 72, 58, 77, 68, 54, 74, np.nan, 76, 56, 78, 53],\n",
    "        'bmi': [22.86, 20.20, 23.15, 20.31, 22.72, 20.54, 23.24, 23.53, 20.32, 23.88, np.nan, 23.72, 20.09, 23.81, 19.71],\n",
    "        'cholesterol_mgdl': [180, 160, 195, 155, 185, 165, 200, 175, 150, 190, 170, 192, 158, 198, 152],\n",
    "        'blood_pressure': ['120/80', '118/78', '125/82', '116/76', '122/81', '119/79', '126/83', \n",
    "                          '121/80', '117/77', '124/82', '118/78', '125/81', '119/79', '127/84', '115/75'],\n",
    "        'glucose_level': [95, 88, 102, 85, 96, 89, 105, 92, 87, 101, 90, 100, 86, 104, 84],\n",
    "        'smoker': ['No', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'No'],\n",
    "        'alcohol_consumer': ['Occasional', 'Never', 'Regular', 'Never', 'Occasional', 'Never', \n",
    "                            'Regular', 'Occasional', 'Never', 'Regular', 'Never', 'Occasional', 'Never', 'Regular', 'Never'],\n",
    "        'physical_activity': ['High', 'Medium', 'Low', 'High', 'Medium', 'High', 'Low', \n",
    "                            'Medium', 'High', 'Low', 'Medium', 'High', 'Medium', 'Low', 'High'],\n",
    "        'stress_level': [3, 2, 4, 1, 3, 2, 5, 3, 2, 4, 2, 3, 1, 4, 2],\n",
    "        'sleep_hours': [7.5, 8.0, 6.0, 8.5, 7.0, 8.0, 5.5, 7.0, 8.5, 6.0, 8.0, 7.5, 9.0, 6.5, 8.0],\n",
    "        'gpa': [3.8, 3.9, 3.5, 4.0, 3.7, 3.8, 3.4, 3.6, 3.9, 3.3, 3.8, 3.7, 4.0, 3.2, 3.9]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(medical_data)\n",
    "    df.to_csv('medical_student.csv', index=False)\n",
    "    print(\"✓ Sample 'medical_student.csv' created with 15 records\")\n",
    "else:\n",
    "    # Load existing dataset\n",
    "    df = pd.read_csv('medical_student.csv')\n",
    "    print(\"✓ 'medical_student.csv' loaded successfully!\")\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 2: Check info and identify columns with missing values\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"TASK 2: Checking DataFrame Information and Missing Values\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n2.1 Basic DataFrame Info:\")\n",
    "print(\"-\" * 40)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\\n2.2 Statistical Summary:\")\n",
    "print(\"-\" * 40)\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "print(\"\\n\\n2.3 Columns with Missing (Null) Values:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "\n",
    "# Create a DataFrame for better display\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "# Filter only columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"Found {len(missing_df)} columns with missing values:\")\n",
    "    print(missing_df.to_string(index=False))\n",
    "    \n",
    "    # Show detailed info about columns with missing values\n",
    "    print(\"\\nDetailed information about columns with missing values:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in missing_df['Column']:\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(f\"  Data type: {df[col].dtype}\")\n",
    "        print(f\"  Missing values: {missing_df[missing_df['Column'] == col]['Missing_Count'].values[0]}\")\n",
    "        print(f\"  Percentage missing: {missing_df[missing_df['Column'] == col]['Missing_Percentage'].values[0]}%\")\n",
    "        print(f\"  Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "else:\n",
    "    print(\"✓ No missing values found in the dataset!\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 3: Fill missing values using various techniques\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"TASK 3: Handling Missing Values\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a copy of the original DataFrame for imputation\n",
    "df_filled = df.copy()\n",
    "\n",
    "print(\"\\nOriginal dataset missing values:\")\n",
    "original_missing = df_filled.isnull().sum().sum()\n",
    "print(f\"Total missing values: {original_missing}\")\n",
    "\n",
    "if original_missing > 0:\n",
    "    print(\"\\nFilling missing values using different techniques:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # For each column with missing values, apply appropriate imputation\n",
    "    for col in missing_df['Column']:\n",
    "        missing_count = missing_df[missing_df['Column'] == col]['Missing_Count'].values[0]\n",
    "        missing_pct = missing_df[missing_df['Column'] == col]['Missing_Percentage'].values[0]\n",
    "        \n",
    "        print(f\"\\n--- Column: {col} ---\")\n",
    "        print(f\"Missing: {missing_count} values ({missing_pct}%)\")\n",
    "        print(f\"Data type: {df_filled[col].dtype}\")\n",
    "        \n",
    "        # Different imputation strategies based on column type and characteristics\n",
    "        if df_filled[col].dtype == 'object':\n",
    "            # For categorical/text columns\n",
    "            if col == 'name':\n",
    "                # For names, use a placeholder\n",
    "                df_filled[col] = df_filled[col].fillna('Unknown')\n",
    "                print(\"Method: Filled with 'Unknown' (placeholder for missing names)\")\n",
    "                print(\"Reason: Names are unique identifiers, so we use a placeholder\")\n",
    "            else:\n",
    "                # For other categorical columns, use mode\n",
    "                mode_value = df_filled[col].mode()[0] if not df_filled[col].mode().empty else 'Unknown'\n",
    "                df_filled[col] = df_filled[col].fillna(mode_value)\n",
    "                print(f\"Method: Filled with mode '{mode_value}'\")\n",
    "                print(f\"Reason: Most frequent value is appropriate for categorical data\")\n",
    "        \n",
    "        elif pd.api.types.is_numeric_dtype(df_filled[col]):\n",
    "            # For numerical columns\n",
    "            if missing_pct < 10:\n",
    "                # If few missing values, use mean for normal distributions\n",
    "                mean_value = df_filled[col].mean()\n",
    "                df_filled[col] = df_filled[col].fillna(mean_value)\n",
    "                print(f\"Method: Filled with mean {mean_value:.2f}\")\n",
    "                print(f\"Reason: Less than 10% missing, mean preserves overall distribution\")\n",
    "            elif 10 <= missing_pct <= 30:\n",
    "                # If moderate missing values, use median (robust to outliers)\n",
    "                median_value = df_filled[col].median()\n",
    "                df_filled[col] = df_filled[col].fillna(median_value)\n",
    "                print(f\"Method: Filled with median {median_value:.2f}\")\n",
    "                print(f\"Reason: Moderate missing values, median is robust to outliers\")\n",
    "            else:\n",
    "                # If many missing values, use interpolation or forward fill\n",
    "                df_filled[col] = df_filled[col].interpolate(method='linear')\n",
    "                print(f\"Method: Linear interpolation\")\n",
    "                print(f\"Reason: High percentage missing, interpolation maintains trend\")\n",
    "            \n",
    "            # For BMI specifically, we can calculate it from height and weight if available\n",
    "            if col == 'bmi' and 'weight_kg' in df_filled.columns and 'height_cm' in df_filled.columns:\n",
    "                # Calculate BMI for rows where it's missing but height and weight are available\n",
    "                mask = df_filled['bmi'].isnull() & df_filled['height_cm'].notnull() & df_filled['weight_kg'].notnull()\n",
    "                df_filled.loc[mask, 'bmi'] = df_filled.loc[mask, 'weight_kg'] / ((df_filled.loc[mask, 'height_cm']/100) ** 2)\n",
    "                print(\"Additional Method: Calculated from height and weight where possible\")\n",
    "        \n",
    "        # Show before and after sample\n",
    "        print(f\"Sample values after imputation: {df_filled[col].unique()[:3]}\")\n",
    "\n",
    "else:\n",
    "    print(\"No missing values to fill!\")\n",
    "\n",
    "print(\"\\n\\nMissing values after imputation:\")\n",
    "print(\"-\" * 40)\n",
    "missing_after = df_filled.isnull().sum()\n",
    "if missing_after.sum() == 0:\n",
    "    print(\"✓ All missing values have been successfully filled!\")\n",
    "else:\n",
    "    print(f\"Remaining missing values: {missing_after.sum()}\")\n",
    "    for col in df_filled.columns[df_filled.isnull().any()]:\n",
    "        print(f\"  {col}: {df_filled[col].isnull().sum()} missing\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 4: Check and manage duplicate values\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"TASK 4: Handling Duplicate Values\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n4.1 Checking for duplicate rows:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_count = df_filled.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nDuplicate rows found:\")\n",
    "    duplicates = df_filled[df_filled.duplicated(keep=False)]  # Keep all duplicates for display\n",
    "    print(f\"Total duplicate entries (including originals): {len(duplicates)}\")\n",
    "    print(\"\\nSample of duplicate rows:\")\n",
    "    print(duplicates.head())\n",
    "    \n",
    "    # Check for duplicates based on specific columns (like student_id which should be unique)\n",
    "    print(\"\\n4.2 Checking for duplicates in key columns:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    key_columns = ['student_id']  # Assuming student_id should be unique\n",
    "    for col in key_columns:\n",
    "        if col in df_filled.columns:\n",
    "            duplicate_ids = df_filled[col].duplicated().sum()\n",
    "            print(f\"Duplicate {col} values: {duplicate_ids}\")\n",
    "    \n",
    "    print(\"\\n4.3 Managing duplicates:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    original_shape = df_filled.shape\n",
    "    df_clean = df_filled.drop_duplicates()\n",
    "    removed_count = original_shape[0] - df_clean.shape[0]\n",
    "    \n",
    "    print(f\"Removed {removed_count} duplicate rows\")\n",
    "    print(f\"Dataset before: {original_shape}\")\n",
    "    print(f\"Dataset after: {df_clean.shape}\")\n",
    "    \n",
    "    # Also check for near-duplicates based on multiple columns\n",
    "    print(\"\\n4.4 Checking for potential near-duplicates:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check for rows that might be duplicates based on multiple columns\n",
    "    duplicate_cols = ['name', 'age', 'gender']\n",
    "    if all(col in df_clean.columns for col in duplicate_cols):\n",
    "        potential_dups = df_clean.duplicated(subset=duplicate_cols, keep=False).sum()\n",
    "        print(f\"Potential duplicates based on {duplicate_cols}: {potential_dups}\")\n",
    "        \n",
    "        if potential_dups > 0:\n",
    "            print(\"These might represent the same student entered multiple times\")\n",
    "            sample_dups = df_clean[df_clean.duplicated(subset=duplicate_cols, keep=False)]\n",
    "            print(f\"\\nSample of potential duplicates:\")\n",
    "            print(sample_dups.head(6))\n",
    "else:\n",
    "    print(\"✓ No duplicate rows found!\")\n",
    "    df_clean = df_filled.copy()\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. Dataset Overview:\")\n",
    "print(f\"   • Original dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"   • After cleaning: {df_clean.shape[0]} rows, {df_clean.shape[1]} columns\")\n",
    "\n",
    "print(f\"\\n2. Missing Values Handling:\")\n",
    "print(f\"   • Original missing values: {original_missing}\")\n",
    "print(f\"   • Missing values after imputation: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\n3. Duplicate Handling:\")\n",
    "print(f\"   • Duplicate rows found: {duplicate_count}\")\n",
    "print(f\"   • Rows removed: {df.shape[0] - df_clean.shape[0]}\")\n",
    "\n",
    "print(f\"\\n4. Data Quality Check:\")\n",
    "print(f\"   • Total records: {len(df_clean)}\")\n",
    "print(f\"   • Complete records: {len(df_clean.dropna())}\")\n",
    "print(f\"   • Data completeness: {(len(df_clean.dropna()) / len(df_clean) * 100):.1f}%\")\n",
    "\n",
    "print(\"\\n5. First 3 rows of cleaned dataset:\")\n",
    "print(\"-\" * 40)\n",
    "print(df_clean.head(3))\n",
    "\n",
    "print(\"\\n6. Dataset Info:\")\n",
    "print(\"-\" * 40)\n",
    "df_clean.info()\n",
    "\n",
    "# Save cleaned dataset (optional)\n",
    "df_clean.to_csv('medical_student_cleaned.csv', index=False)\n",
    "print(f\"\\n✓ Cleaned dataset saved as 'medical_student_cleaned.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL TASKS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f9be6f1-c828-47fa-9100-ea1840b61248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TITANIC DATASET ANALYSIS - PROBLEM 1\n",
      "======================================================================\n",
      "\n",
      "STEP 1: Loading 'titanic.csv' dataset\n",
      "--------------------------------------------------\n",
      "✗ ERROR: 'titanic.csv' not found!\n",
      "\n",
      "Creating a sample titanic dataset...\n",
      "✓ Sample 'titanic.csv' created with 10 records\n",
      "\n",
      "Dataset Shape: 10 rows × 12 columns\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Dataset columns:\n",
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Creating subset with required columns\n",
      "======================================================================\n",
      "\n",
      "Required columns: ['Name', 'Pclass', 'Sex', 'Age', 'Fare', 'Survived']\n",
      "Available columns: ['Name', 'Pclass', 'Sex', 'Age', 'Fare', 'Survived']\n",
      "\n",
      "✓ Created subset with 6 columns:\n",
      "  Columns: ['Name', 'Pclass', 'Sex', 'Age', 'Fare', 'Survived']\n",
      "  Subset shape: (10, 6)\n",
      "\n",
      "First 5 rows of subset:\n",
      "                                                Name  Pclass     Sex   Age  \\\n",
      "0                            Braund, Mr. Owen Harris       3    male  22.0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...       1  female  38.0   \n",
      "2                             Heikkinen, Miss. Laina       3  female  26.0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)       1  female  35.0   \n",
      "4                           Allen, Mr. William Henry       3    male  35.0   \n",
      "\n",
      "      Fare  Survived  \n",
      "0   7.2500         0  \n",
      "1  71.2833         1  \n",
      "2   7.9250         1  \n",
      "3  53.1000         1  \n",
      "4   8.0500         0  \n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Filtering for first-class passengers (Pclass == 1)\n",
      "======================================================================\n",
      "\n",
      "Total passengers in dataset: 10\n",
      "First-class passengers (Pclass == 1): 3\n",
      "\n",
      "✓ Successfully filtered 3 first-class passengers\n",
      "\n",
      "First 5 first-class passengers:\n",
      "                                                Name  Pclass     Sex   Age  \\\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...       1  female  38.0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)       1  female  35.0   \n",
      "6                            McCarthy, Mr. Timothy J       1    male  54.0   \n",
      "\n",
      "      Fare  Survived  \n",
      "1  71.2833         1  \n",
      "3  53.1000         1  \n",
      "6  51.8625         0  \n",
      "\n",
      "Summary of first-class passengers:\n",
      "  Male: 1\n",
      "  Female: 2\n",
      "  Survived: 2 (66.7%)\n",
      "  Did not survive: 1\n",
      "  Average age: 42.3 years\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Fare Statistics for First-Class Passengers\n",
      "======================================================================\n",
      "\n",
      "Fare Statistics for First-Class Passengers:\n",
      "----------------------------------------\n",
      "Mean Fare                : $58.75\n",
      "Median Fare              : $53.10\n",
      "Maximum Fare             : $71.28\n",
      "Minimum Fare             : $51.86\n",
      "Standard Deviation       : 10.872988532597649\n",
      "Total Passengers         : 3\n",
      "Passengers with Fare data: $3.00\n",
      "\n",
      "Additional Fare Analysis:\n",
      "----------------------------------------\n",
      "\n",
      "Fare Distribution:\n",
      "  Q1 (25th percentile): $52.48\n",
      "  Q3 (75th percentile): $62.19\n",
      "  IQR: $9.71\n",
      "\n",
      "Passenger with highest fare ($71.28):\n",
      "  Name: Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n",
      "  Survived: Yes\n",
      "\n",
      "Passenger with lowest fare ($51.86):\n",
      "  Name: McCarthy, Mr. Timothy J\n",
      "  Survived: No\n",
      "\n",
      "Average Fare by Gender:\n",
      "         mean  count\n",
      "Sex                 \n",
      "female  62.19      2\n",
      "male    51.86      1\n",
      "\n",
      "Fare Range Visualization:\n",
      "\n",
      "$51.86                                     MIN\n",
      "$58.75                                          MEAN\n",
      "$53.10                                      MEDIAN\n",
      "$71.28                                                   MAX\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY - PROBLEM 1\n",
      "======================================================================\n",
      "\n",
      "1. Dataset Overview:\n",
      "   • Total passengers: 10\n",
      "   • Subset columns: 6 of 6 requested\n",
      "   • First-class passengers: 3\n",
      "\n",
      "2. Fare Statistics for First-Class Passengers:\n",
      "   • Mean Fare:   $58.75\n",
      "   • Median Fare: $53.10\n",
      "   • Maximum Fare: $71.28\n",
      "   • Minimum Fare: $51.86\n",
      "   • Fare Range: $19.42\n",
      "\n",
      "3. Data Quality:\n",
      "   • Missing fare values in first-class: 0\n",
      "   • Data completeness: 100.0%\n",
      "\n",
      "4. Sample of First-Class Passengers:\n",
      "----------------------------------------\n",
      "                                                Name  Pclass     Sex   Age  \\\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...       1  female  38.0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)       1  female  35.0   \n",
      "6                            McCarthy, Mr. Timothy J       1    male  54.0   \n",
      "\n",
      "     Fare  Survived  \n",
      "1  $71.28         1  \n",
      "3  $53.10         1  \n",
      "6  $51.86         0  \n",
      "\n",
      "======================================================================\n",
      "PROBLEM 1 COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TITANIC DATASET ANALYSIS - PROBLEM 1\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load and check the dataset\n",
    "# ============================================================================\n",
    "print(\"\\nSTEP 1: Loading 'titanic.csv' dataset\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists('titanic.csv'):\n",
    "    print(\"✗ ERROR: 'titanic.csv' not found!\")\n",
    "    print(\"\\nCreating a sample titanic dataset...\")\n",
    "    \n",
    "    # Create sample Titanic data\n",
    "    titanic_data = {\n",
    "        'PassengerId': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
    "        'Pclass': [3, 1, 3, 1, 3, 3, 1, 3, 3, 2],\n",
    "        'Name': ['Braund, Mr. Owen Harris', \n",
    "                'Cumings, Mrs. John Bradley (Florence Briggs Thayer)',\n",
    "                'Heikkinen, Miss. Laina',\n",
    "                'Futrelle, Mrs. Jacques Heath (Lily May Peel)',\n",
    "                'Allen, Mr. William Henry',\n",
    "                'Moran, Mr. James',\n",
    "                'McCarthy, Mr. Timothy J',\n",
    "                'Palsson, Master. Gosta Leonard',\n",
    "                'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)',\n",
    "                'Nasser, Mrs. Nicholas (Adele Achem)'],\n",
    "        'Sex': ['male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'female'],\n",
    "        'Age': [22.0, 38.0, 26.0, 35.0, 35.0, np.nan, 54.0, 2.0, 27.0, 14.0],\n",
    "        'SibSp': [1, 1, 0, 1, 0, 0, 0, 3, 0, 1],\n",
    "        'Parch': [0, 0, 0, 0, 0, 0, 0, 1, 2, 0],\n",
    "        'Ticket': ['A/5 21171', 'PC 17599', 'STON/O2. 3101282', '113803', '373450', '330877', \n",
    "                  '17463', '349909', '347742', '237736'],\n",
    "        'Fare': [7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708],\n",
    "        'Cabin': [np.nan, 'C85', np.nan, 'C123', np.nan, np.nan, 'E46', np.nan, np.nan, np.nan],\n",
    "        'Embarked': ['S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'C']\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(titanic_data)\n",
    "    df.to_csv('titanic.csv', index=False)\n",
    "    print(\"✓ Sample 'titanic.csv' created with 10 records\")\n",
    "else:\n",
    "    # Load the dataset\n",
    "    try:\n",
    "        df = pd.read_csv('titanic.csv')\n",
    "        print(\"✓ 'titanic.csv' loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading file: {e}\")\n",
    "        exit()\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Create subset with required columns\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: Creating subset with required columns\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# List of required columns\n",
    "required_columns = ['Name', 'Pclass', 'Sex', 'Age', 'Fare', 'Survived']\n",
    "\n",
    "# Check which columns exist in the dataset\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "print(f\"\\nRequired columns: {required_columns}\")\n",
    "print(f\"Available columns: {available_columns}\")\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"⚠ Missing columns: {missing_columns}\")\n",
    "    print(\"Will use available columns only...\")\n",
    "\n",
    "# Create subset DataFrame\n",
    "if available_columns:\n",
    "    df_subset = df[available_columns].copy()\n",
    "    print(f\"\\n✓ Created subset with {len(available_columns)} columns:\")\n",
    "    print(f\"  Columns: {available_columns}\")\n",
    "    print(f\"  Subset shape: {df_subset.shape}\")\n",
    "    \n",
    "    print(\"\\nFirst 5 rows of subset:\")\n",
    "    print(df_subset.head())\n",
    "else:\n",
    "    print(\"✗ ERROR: None of the required columns are available!\")\n",
    "    exit()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Filter for first-class passengers (Pclass == 1)\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: Filtering for first-class passengers (Pclass == 1)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'Pclass' in df_subset.columns:\n",
    "    # Filter for first-class passengers\n",
    "    first_class_df = df_subset[df_subset['Pclass'] == 1].copy()\n",
    "    \n",
    "    print(f\"\\nTotal passengers in dataset: {len(df_subset)}\")\n",
    "    print(f\"First-class passengers (Pclass == 1): {len(first_class_df)}\")\n",
    "    \n",
    "    if len(first_class_df) > 0:\n",
    "        print(f\"\\n✓ Successfully filtered {len(first_class_df)} first-class passengers\")\n",
    "        \n",
    "        print(\"\\nFirst 5 first-class passengers:\")\n",
    "        print(first_class_df.head())\n",
    "        \n",
    "        print(\"\\nSummary of first-class passengers:\")\n",
    "        print(f\"  Male: {len(first_class_df[first_class_df['Sex'] == 'male'])}\")\n",
    "        print(f\"  Female: {len(first_class_df[first_class_df['Sex'] == 'female'])}\")\n",
    "        \n",
    "        if 'Survived' in first_class_df.columns:\n",
    "            survivors = first_class_df['Survived'].sum()\n",
    "            print(f\"  Survived: {survivors} ({survivors/len(first_class_df)*100:.1f}%)\")\n",
    "            print(f\"  Did not survive: {len(first_class_df) - survivors}\")\n",
    "        \n",
    "        if 'Age' in first_class_df.columns:\n",
    "            print(f\"  Average age: {first_class_df['Age'].mean():.1f} years\")\n",
    "    else:\n",
    "        print(\"⚠ No first-class passengers found in the dataset!\")\n",
    "else:\n",
    "    print(\"✗ ERROR: 'Pclass' column not available in subset!\")\n",
    "    exit()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Calculate Fare statistics for first-class passengers\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: Fare Statistics for First-Class Passengers\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'Fare' in first_class_df.columns and len(first_class_df) > 0:\n",
    "    # Get fare statistics\n",
    "    fare_stats = {\n",
    "        'Mean Fare': first_class_df['Fare'].mean(),\n",
    "        'Median Fare': first_class_df['Fare'].median(),\n",
    "        'Maximum Fare': first_class_df['Fare'].max(),\n",
    "        'Minimum Fare': first_class_df['Fare'].min(),\n",
    "        'Standard Deviation': first_class_df['Fare'].std(),\n",
    "        'Total Passengers': len(first_class_df),\n",
    "        'Passengers with Fare data': first_class_df['Fare'].count()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFare Statistics for First-Class Passengers:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for stat_name, stat_value in fare_stats.items():\n",
    "        if 'Fare' in stat_name:\n",
    "            print(f\"{stat_name:25}: ${stat_value:,.2f}\")\n",
    "        else:\n",
    "            print(f\"{stat_name:25}: {stat_value}\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\nAdditional Fare Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check for missing fare values\n",
    "    missing_fare = first_class_df['Fare'].isnull().sum()\n",
    "    if missing_fare > 0:\n",
    "        print(f\"⚠ Missing fare values: {missing_fare}\")\n",
    "    \n",
    "    # Fare distribution\n",
    "    print(f\"\\nFare Distribution:\")\n",
    "    print(f\"  Q1 (25th percentile): ${first_class_df['Fare'].quantile(0.25):,.2f}\")\n",
    "    print(f\"  Q3 (75th percentile): ${first_class_df['Fare'].quantile(0.75):,.2f}\")\n",
    "    print(f\"  IQR: ${first_class_df['Fare'].quantile(0.75) - first_class_df['Fare'].quantile(0.25):,.2f}\")\n",
    "    \n",
    "    # Identify passengers with extreme fares\n",
    "    max_fare_passenger = first_class_df[first_class_df['Fare'] == fare_stats['Maximum Fare']]\n",
    "    min_fare_passenger = first_class_df[first_class_df['Fare'] == fare_stats['Minimum Fare']]\n",
    "    \n",
    "    if len(max_fare_passenger) > 0 and 'Name' in max_fare_passenger.columns:\n",
    "        print(f\"\\nPassenger with highest fare (${fare_stats['Maximum Fare']:.2f}):\")\n",
    "        print(f\"  Name: {max_fare_passenger['Name'].iloc[0]}\")\n",
    "        if 'Survived' in max_fare_passenger.columns:\n",
    "            survived = 'Yes' if max_fare_passenger['Survived'].iloc[0] == 1 else 'No'\n",
    "            print(f\"  Survived: {survived}\")\n",
    "    \n",
    "    if len(min_fare_passenger) > 0 and 'Name' in min_fare_passenger.columns:\n",
    "        print(f\"\\nPassenger with lowest fare (${fare_stats['Minimum Fare']:.2f}):\")\n",
    "        print(f\"  Name: {min_fare_passenger['Name'].iloc[0]}\")\n",
    "        if 'Survived' in min_fare_passenger.columns:\n",
    "            survived = 'Yes' if min_fare_passenger['Survived'].iloc[0] == 1 else 'No'\n",
    "            print(f\"  Survived: {survived}\")\n",
    "    \n",
    "    # Fare by gender\n",
    "    if 'Sex' in first_class_df.columns:\n",
    "        print(f\"\\nAverage Fare by Gender:\")\n",
    "        fare_by_gender = first_class_df.groupby('Sex')['Fare'].agg(['mean', 'count']).round(2)\n",
    "        print(fare_by_gender)\n",
    "    \n",
    "    # Fare visualization using text\n",
    "    print(f\"\\nFare Range Visualization:\")\n",
    "    fare_min = fare_stats['Minimum Fare']\n",
    "    fare_max = fare_stats['Maximum Fare']\n",
    "    fare_mean = fare_stats['Mean Fare']\n",
    "    fare_median = fare_stats['Median Fare']\n",
    "    \n",
    "    # Simple text-based visualization\n",
    "    scale = 50\n",
    "    min_pos = int((fare_min / fare_max) * scale)\n",
    "    mean_pos = int((fare_mean / fare_max) * scale)\n",
    "    median_pos = int((fare_median / fare_max) * scale)\n",
    "    \n",
    "    print(f\"\\n${fare_min:.2f} {' ' * min_pos}MIN\")\n",
    "    print(f\"${fare_mean:.2f} {' ' * mean_pos}MEAN\")\n",
    "    print(f\"${fare_median:.2f} {' ' * median_pos}MEDIAN\")\n",
    "    print(f\"${fare_max:.2f} {' ' * scale}MAX\")\n",
    "    \n",
    "else:\n",
    "    if len(first_class_df) == 0:\n",
    "        print(\"✗ ERROR: No first-class passengers to analyze!\")\n",
    "    else:\n",
    "        print(\"✗ ERROR: 'Fare' column not available in first-class subset!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Final Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY - PROBLEM 1\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. Dataset Overview:\")\n",
    "print(f\"   • Total passengers: {len(df)}\")\n",
    "print(f\"   • Subset columns: {len(available_columns)} of {len(required_columns)} requested\")\n",
    "print(f\"   • First-class passengers: {len(first_class_df)}\")\n",
    "\n",
    "if 'Fare' in first_class_df.columns and len(first_class_df) > 0:\n",
    "    print(f\"\\n2. Fare Statistics for First-Class Passengers:\")\n",
    "    print(f\"   • Mean Fare:   ${first_class_df['Fare'].mean():.2f}\")\n",
    "    print(f\"   • Median Fare: ${first_class_df['Fare'].median():.2f}\")\n",
    "    print(f\"   • Maximum Fare: ${first_class_df['Fare'].max():.2f}\")\n",
    "    print(f\"   • Minimum Fare: ${first_class_df['Fare'].min():.2f}\")\n",
    "    print(f\"   • Fare Range: ${first_class_df['Fare'].max() - first_class_df['Fare'].min():.2f}\")\n",
    "\n",
    "print(f\"\\n3. Data Quality:\")\n",
    "print(f\"   • Missing fare values in first-class: {first_class_df['Fare'].isnull().sum() if 'Fare' in first_class_df.columns else 'N/A'}\")\n",
    "print(f\"   • Data completeness: {(len(first_class_df.dropna()) / len(first_class_df) * 100 if len(first_class_df) > 0 else 0):.1f}%\")\n",
    "\n",
    "print(\"\\n4. Sample of First-Class Passengers:\")\n",
    "print(\"-\" * 40)\n",
    "if len(first_class_df) > 0:\n",
    "    # Display sample with formatted fare\n",
    "    sample_df = first_class_df.head(3).copy()\n",
    "    if 'Fare' in sample_df.columns:\n",
    "        sample_df['Fare'] = sample_df['Fare'].apply(lambda x: f\"${x:.2f}\")\n",
    "    print(sample_df)\n",
    "else:\n",
    "    print(\"No first-class passengers found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROBLEM 1 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47622073-914a-49e2-9841-aa27bac92672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TITANIC DATASET ANALYSIS - PROBLEM 2\n",
      "======================================================================\n",
      "\n",
      "STEP 1: Loading dataset and creating subset\n",
      "--------------------------------------------------\n",
      "✓ 'titanic.csv' loaded successfully!\n",
      "\n",
      "Original dataset shape: (10, 12)\n",
      "\n",
      "\n",
      "STEP 2: Creating subset DataFrame\n",
      "--------------------------------------------------\n",
      "✓ Created subset with columns: ['Name', 'Pclass', 'Sex', 'Age', 'Fare', 'Survived']\n",
      "Subset shape: (10, 6)\n",
      "\n",
      "\n",
      "STEP 3: Filtering for first-class passengers\n",
      "--------------------------------------------------\n",
      "✓ Found 3 first-class passengers\n",
      "\n",
      "First 5 first-class passengers:\n",
      "                                                Name  Pclass     Sex   Age  \\\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...       1  female  38.0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)       1  female  35.0   \n",
      "6                            McCarthy, Mr. Timothy J       1    male  54.0   \n",
      "\n",
      "      Fare  Survived  \n",
      "1  71.2833         1  \n",
      "3  53.1000         1  \n",
      "6  51.8625         0  \n",
      "\n",
      "\n",
      "======================================================================\n",
      "PROBLEM 2: Null Values in 'Age' Column\n",
      "======================================================================\n",
      "\n",
      "1. Checking for null values in 'Age' column:\n",
      "----------------------------------------\n",
      "Total first-class passengers: 3\n",
      "Null values in 'Age' column: 0\n",
      "Percentage of null values: 0.00%\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Dropping rows with null Age values\n",
      "======================================================================\n",
      "Original DataFrame shape: (3, 6)\n",
      "\n",
      "After dropping rows with null Age:\n",
      "  Rows dropped: 0\n",
      "  New shape: (3, 6)\n",
      "  Remaining passengers: 3\n",
      "\n",
      "Verification:\n",
      "  Null values remaining in 'Age': 0\n",
      "  ✓ All null values removed successfully!\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Before and After Comparison\n",
      "======================================================================\n",
      "\n",
      "1. Age Statistics Comparison:\n",
      "----------------------------------------\n",
      "Before dropping nulls:\n",
      "  Count: 3\n",
      "  Mean: 42.33\n",
      "  Std: 10.21\n",
      "  Min: 35.00\n",
      "  Max: 54.00\n",
      "\n",
      "After dropping nulls:\n",
      "  Count: 3\n",
      "  Mean: 42.33\n",
      "  Std: 10.21\n",
      "  Min: 35.00\n",
      "  Max: 54.00\n",
      "\n",
      "2. Demographic Changes:\n",
      "----------------------------------------\n",
      "Gender distribution:\n",
      "\n",
      "  Before dropping:\n",
      "    female: 2 (66.7%)\n",
      "    male: 1 (33.3%)\n",
      "\n",
      "  After dropping:\n",
      "    female: 2 (66.7%)\n",
      "    male: 1 (33.3%)\n",
      "\n",
      "Survival rate:\n",
      "  Before dropping: 66.7%\n",
      "  After dropping: 66.7%\n",
      "  Change: 0.0%\n",
      "\n",
      "3. Sample of Cleaned DataFrame:\n",
      "----------------------------------------\n",
      "First 5 rows after dropping null Age values:\n",
      "                                                Name  Pclass     Sex   Age  \\\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...       1  female  38.0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)       1  female  35.0   \n",
      "6                            McCarthy, Mr. Timothy J       1    male  54.0   \n",
      "\n",
      "      Fare  Survived  \n",
      "1  71.2833         1  \n",
      "3  53.1000         1  \n",
      "6  51.8625         0  \n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 6: Alternative Approaches to Handle Null Age Values\n",
      "======================================================================\n",
      "\n",
      "Instead of dropping, you could consider:\n",
      "----------------------------------------\n",
      "1. Imputation Methods:\n",
      "   a. Mean/Median Imputation:\n",
      "      - Fill null values with mean or median age\n",
      "      - Simple but may reduce variance\n",
      "\n",
      "   b. Group-based Imputation:\n",
      "      - Fill nulls with mean age by gender, class, etc.\n",
      "      - More accurate than overall mean\n",
      "\n",
      "   c. Regression Imputation:\n",
      "      - Predict age based on other variables\n",
      "      - Most accurate but computationally intensive\n",
      "\n",
      "2. Keep with Indicator:\n",
      "   - Keep null values but add an indicator column\n",
      "   - Useful if missingness is informative\n",
      "\n",
      "3. Multiple Imputation:\n",
      "   - Create multiple datasets with different imputed values\n",
      "   - Accounts for uncertainty in imputation\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY - PROBLEM 2\n",
      "======================================================================\n",
      "\n",
      "1. Null Value Analysis:\n",
      "   • Total first-class passengers: 3\n",
      "   • Null values in 'Age' column: 0\n",
      "   • Percentage null: 0.00%\n",
      "\n",
      "2. Data Cleaning Results:\n",
      "   • Rows dropped: 0\n",
      "   • Remaining passengers: 3\n",
      "   • Data loss: 0.0%\n",
      "\n",
      "3. Cleaned Dataset Information:\n",
      "   • Shape: (3, 6)\n",
      "   • Age range: 35.0 to 54.0\n",
      "   • Average age: 42.3\n",
      "   • Survived: 2 (66.7%)\n",
      "\n",
      "4. Sample of Cleaned Data:\n",
      "----------------------------------------\n",
      "                                                Name  Pclass     Sex   Age  \\\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...       1  female  38.0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)       1  female  35.0   \n",
      "6                            McCarthy, Mr. Timothy J       1    male  54.0   \n",
      "\n",
      "     Fare  Survived  \n",
      "1  $71.28         1  \n",
      "3  $53.10         1  \n",
      "6  $51.86         0  \n",
      "\n",
      "======================================================================\n",
      "PROBLEM 2 COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Key Answer:\n",
      "----------------------------------------\n",
      "Number of null values in 'Age' column: 0\n",
      "Rows dropped: 0\n",
      "Final DataFrame shape after dropping: (3, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TITANIC DATASET ANALYSIS - PROBLEM 2\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load the dataset and create the subset (from Problem 1)\n",
    "# ============================================================================\n",
    "print(\"\\nSTEP 1: Loading dataset and creating subset\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load or create the dataset\n",
    "if not os.path.exists('titanic.csv'):\n",
    "    print(\"Creating sample Titanic dataset...\")\n",
    "    titanic_data = {\n",
    "        'PassengerId': list(range(1, 31)),\n",
    "        'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0],\n",
    "        'Pclass': [3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 1, 3],\n",
    "        'Name': [f'Passenger {i}' for i in range(1, 31)],\n",
    "        'Sex': ['male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'female',\n",
    "               'male', 'female', 'male', 'female', 'male', 'female', 'female', 'male', 'male', 'female',\n",
    "               'male', 'female', 'male', 'female', 'female', 'male', 'female', 'male', 'female', 'male'],\n",
    "        'Age': [22.0, 38.0, 26.0, 35.0, 35.0, np.nan, 54.0, 2.0, 27.0, 14.0,\n",
    "               45.0, np.nan, 20.0, 28.0, np.nan, 40.0, 30.0, 25.0, 60.0, 18.0,\n",
    "               np.nan, 32.0, 22.0, np.nan, 29.0, 19.0, 36.0, 24.0, np.nan, 50.0],\n",
    "        'SibSp': [1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "        'Parch': [0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
    "        'Ticket': [f'Ticket{i}' for i in range(1, 31)],\n",
    "        'Fare': [7.25, 71.28, 7.93, 53.10, 8.05, 8.46, 51.86, 21.08, 11.13, 30.07,\n",
    "                80.00, 25.00, 7.50, 90.00, 15.00, 75.00, 35.00, 8.00, 60.00, 40.00,\n",
    "                10.00, 85.00, 20.00, 95.00, 45.00, 9.00, 70.00, 22.00, 100.00, 12.00],\n",
    "        'Cabin': [None, 'C85', None, 'C123', None, None, 'E46', None, None, None] * 3,\n",
    "        'Embarked': ['S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'C'] * 3\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(titanic_data)\n",
    "    df.to_csv('titanic.csv', index=False)\n",
    "    print(\"✓ Sample 'titanic.csv' created with 30 records\")\n",
    "else:\n",
    "    df = pd.read_csv('titanic.csv')\n",
    "    print(\"✓ 'titanic.csv' loaded successfully!\")\n",
    "\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Create subset with required columns (from Problem 1)\n",
    "# ============================================================================\n",
    "print(\"\\n\\nSTEP 2: Creating subset DataFrame\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "required_columns = ['Name', 'Pclass', 'Sex', 'Age', 'Fare', 'Survived']\n",
    "available_columns = [col for col in required_columns if col in df.columns]\n",
    "\n",
    "if len(available_columns) < 3:\n",
    "    print(\"✗ ERROR: Not enough required columns found!\")\n",
    "    exit()\n",
    "\n",
    "df_subset = df[available_columns].copy()\n",
    "print(f\"✓ Created subset with columns: {available_columns}\")\n",
    "print(f\"Subset shape: {df_subset.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Filter for first-class passengers (Pclass == 1)\n",
    "# ============================================================================\n",
    "print(\"\\n\\nSTEP 3: Filtering for first-class passengers\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'Pclass' in df_subset.columns:\n",
    "    first_class_df = df_subset[df_subset['Pclass'] == 1].copy()\n",
    "    print(f\"✓ Found {len(first_class_df)} first-class passengers\")\n",
    "    \n",
    "    print(\"\\nFirst 5 first-class passengers:\")\n",
    "    print(first_class_df.head())\n",
    "else:\n",
    "    print(\"✗ ERROR: 'Pclass' column not found!\")\n",
    "    exit()\n",
    "\n",
    "# ============================================================================\n",
    "# PROBLEM 2: Analyze and handle null values in 'Age' column\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"PROBLEM 2: Null Values in 'Age' Column\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if 'Age' column exists\n",
    "if 'Age' not in first_class_df.columns:\n",
    "    print(\"✗ ERROR: 'Age' column not found in the subset!\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n1. Checking for null values in 'Age' column:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Count null values\n",
    "null_count = first_class_df['Age'].isnull().sum()\n",
    "total_rows = len(first_class_df)\n",
    "null_percentage = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "\n",
    "print(f\"Total first-class passengers: {total_rows}\")\n",
    "print(f\"Null values in 'Age' column: {null_count}\")\n",
    "print(f\"Percentage of null values: {null_percentage:.2f}%\")\n",
    "\n",
    "# Show detailed information about null values\n",
    "if null_count > 0:\n",
    "    print(f\"\\n2. Detailed analysis of null values:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get rows with null Age\n",
    "    null_age_rows = first_class_df[first_class_df['Age'].isnull()]\n",
    "    \n",
    "    print(f\"Rows with null Age values ({null_count}):\")\n",
    "    print(null_age_rows[['Name', 'Sex', 'Fare', 'Survived']].head())\n",
    "    \n",
    "    # Show summary statistics for rows with null Age\n",
    "    print(f\"\\nSummary of passengers with null Age:\")\n",
    "    if 'Sex' in null_age_rows.columns:\n",
    "        gender_counts = null_age_rows['Sex'].value_counts()\n",
    "        print(f\"Gender distribution: {gender_counts.to_dict()}\")\n",
    "    \n",
    "    if 'Survived' in null_age_rows.columns:\n",
    "        survival_counts = null_age_rows['Survived'].value_counts()\n",
    "        survived = survival_counts.get(1, 0)\n",
    "        print(f\"Survived: {survived} / Did not survive: {survived - len(null_age_rows)}\")\n",
    "    \n",
    "    # Compare with overall statistics\n",
    "    print(f\"\\n3. Comparison with overall first-class passengers:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"Age statistics for all first-class passengers:\")\n",
    "    print(f\"  Mean age: {first_class_df['Age'].mean():.2f}\")\n",
    "    print(f\"  Median age: {first_class_df['Age'].median():.2f}\")\n",
    "    print(f\"  Age range: {first_class_df['Age'].min():.2f} to {first_class_df['Age'].max():.2f}\")\n",
    "    \n",
    "    print(\"\\nAge statistics for passengers with known Age:\")\n",
    "    known_age_df = first_class_df[first_class_df['Age'].notnull()]\n",
    "    print(f\"  Mean age: {known_age_df['Age'].mean():.2f}\")\n",
    "    print(f\"  Median age: {known_age_df['Age'].median():.2f}\")\n",
    "    print(f\"  Age range: {known_age_df['Age'].min():.2f} to {known_age_df['Age'].max():.2f}\")\n",
    "    \n",
    "    # Check if null values are random or systematic\n",
    "    print(f\"\\n4. Pattern analysis of null values:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check correlation with other variables\n",
    "    if 'Fare' in first_class_df.columns and 'Survived' in first_class_df.columns:\n",
    "        print(\"Average fare by Age availability:\")\n",
    "        fare_by_age_status = first_class_df.groupby(first_class_df['Age'].isnull())['Fare'].mean()\n",
    "        print(f\"  Known Age: ${fare_by_age_status[False]:.2f}\")\n",
    "        print(f\"  Null Age: ${fare_by_age_status[True]:.2f}\")\n",
    "        \n",
    "        print(\"\\nSurvival rate by Age availability:\")\n",
    "        survival_by_age_status = first_class_df.groupby(first_class_df['Age'].isnull())['Survived'].mean()\n",
    "        print(f\"  Known Age: {survival_by_age_status[False]*100:.1f}%\")\n",
    "        print(f\"  Null Age: {survival_by_age_status[True]*100:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Drop rows with null Age values\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: Dropping rows with null Age values\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Store original shape\n",
    "original_shape = first_class_df.shape\n",
    "print(f\"Original DataFrame shape: {original_shape}\")\n",
    "\n",
    "# Create a copy before dropping\n",
    "df_before_drop = first_class_df.copy()\n",
    "\n",
    "# Drop rows with null Age values\n",
    "first_class_clean = first_class_df.dropna(subset=['Age'])\n",
    "\n",
    "# Calculate dropped rows\n",
    "dropped_rows = original_shape[0] - len(first_class_clean)\n",
    "print(f\"\\nAfter dropping rows with null Age:\")\n",
    "print(f\"  Rows dropped: {dropped_rows}\")\n",
    "print(f\"  New shape: {first_class_clean.shape}\")\n",
    "print(f\"  Remaining passengers: {len(first_class_clean)}\")\n",
    "\n",
    "# Verify that all null values are gone\n",
    "remaining_nulls = first_class_clean['Age'].isnull().sum()\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Null values remaining in 'Age': {remaining_nulls}\")\n",
    "print(f\"  ✓ All null values removed successfully!\" if remaining_nulls == 0 else f\"  ⚠ {remaining_nulls} null values remain\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Compare before and after dropping\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: Before and After Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. Age Statistics Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Before dropping nulls:\")\n",
    "print(f\"  Count: {df_before_drop['Age'].count()}\")\n",
    "print(f\"  Mean: {df_before_drop['Age'].mean():.2f}\")\n",
    "print(f\"  Std: {df_before_drop['Age'].std():.2f}\")\n",
    "print(f\"  Min: {df_before_drop['Age'].min():.2f}\")\n",
    "print(f\"  Max: {df_before_drop['Age'].max():.2f}\")\n",
    "\n",
    "print(\"\\nAfter dropping nulls:\")\n",
    "print(f\"  Count: {first_class_clean['Age'].count()}\")\n",
    "print(f\"  Mean: {first_class_clean['Age'].mean():.2f}\")\n",
    "print(f\"  Std: {first_class_clean['Age'].std():.2f}\")\n",
    "print(f\"  Min: {first_class_clean['Age'].min():.2f}\")\n",
    "print(f\"  Max: {first_class_clean['Age'].max():.2f}\")\n",
    "\n",
    "print(\"\\n2. Demographic Changes:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'Sex' in first_class_clean.columns:\n",
    "    print(\"Gender distribution:\")\n",
    "    gender_before = df_before_drop['Sex'].value_counts()\n",
    "    gender_after = first_class_clean['Sex'].value_counts()\n",
    "    \n",
    "    print(f\"\\n  Before dropping:\")\n",
    "    for gender, count in gender_before.items():\n",
    "        print(f\"    {gender}: {count} ({count/len(df_before_drop)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n  After dropping:\")\n",
    "    for gender, count in gender_after.items():\n",
    "        print(f\"    {gender}: {count} ({count/len(first_class_clean)*100:.1f}%)\")\n",
    "\n",
    "if 'Survived' in first_class_clean.columns:\n",
    "    print(\"\\nSurvival rate:\")\n",
    "    survival_before = df_before_drop['Survived'].mean() * 100\n",
    "    survival_after = first_class_clean['Survived'].mean() * 100\n",
    "    print(f\"  Before dropping: {survival_before:.1f}%\")\n",
    "    print(f\"  After dropping: {survival_after:.1f}%\")\n",
    "    print(f\"  Change: {survival_after - survival_before:.1f}%\")\n",
    "\n",
    "print(\"\\n3. Sample of Cleaned DataFrame:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"First 5 rows after dropping null Age values:\")\n",
    "print(first_class_clean.head())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Alternative approaches (for reference)\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"STEP 6: Alternative Approaches to Handle Null Age Values\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nInstead of dropping, you could consider:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"1. Imputation Methods:\")\n",
    "print(\"   a. Mean/Median Imputation:\")\n",
    "print(\"      - Fill null values with mean or median age\")\n",
    "print(\"      - Simple but may reduce variance\")\n",
    "\n",
    "print(\"\\n   b. Group-based Imputation:\")\n",
    "print(\"      - Fill nulls with mean age by gender, class, etc.\")\n",
    "print(\"      - More accurate than overall mean\")\n",
    "\n",
    "print(\"\\n   c. Regression Imputation:\")\n",
    "print(\"      - Predict age based on other variables\")\n",
    "print(\"      - Most accurate but computationally intensive\")\n",
    "\n",
    "print(\"\\n2. Keep with Indicator:\")\n",
    "print(\"   - Keep null values but add an indicator column\")\n",
    "print(\"   - Useful if missingness is informative\")\n",
    "\n",
    "print(\"\\n3. Multiple Imputation:\")\n",
    "print(\"   - Create multiple datasets with different imputed values\")\n",
    "print(\"   - Accounts for uncertainty in imputation\")\n",
    "\n",
    "# Demonstrate mean imputation (as an alternative)\n",
    "if null_count > 0:\n",
    "    df_mean_imputed = df_before_drop.copy()\n",
    "    mean_age = df_mean_imputed['Age'].mean()\n",
    "    df_mean_imputed['Age'] = df_mean_imputed['Age'].fillna(mean_age)\n",
    "    \n",
    "    print(f\"\\nExample of mean imputation:\")\n",
    "    print(f\"  Filled {null_count} null values with mean age: {mean_age:.2f}\")\n",
    "    print(f\"  New mean age: {df_mean_imputed['Age'].mean():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY - PROBLEM 2\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. Null Value Analysis:\")\n",
    "print(f\"   • Total first-class passengers: {total_rows}\")\n",
    "print(f\"   • Null values in 'Age' column: {null_count}\")\n",
    "print(f\"   • Percentage null: {null_percentage:.2f}%\")\n",
    "\n",
    "print(f\"\\n2. Data Cleaning Results:\")\n",
    "print(f\"   • Rows dropped: {dropped_rows}\")\n",
    "print(f\"   • Remaining passengers: {len(first_class_clean)}\")\n",
    "print(f\"   • Data loss: {(dropped_rows/total_rows*100):.1f}%\")\n",
    "\n",
    "print(f\"\\n3. Cleaned Dataset Information:\")\n",
    "print(f\"   • Shape: {first_class_clean.shape}\")\n",
    "print(f\"   • Age range: {first_class_clean['Age'].min():.1f} to {first_class_clean['Age'].max():.1f}\")\n",
    "print(f\"   • Average age: {first_class_clean['Age'].mean():.1f}\")\n",
    "\n",
    "if 'Survived' in first_class_clean.columns:\n",
    "    survived_count = first_class_clean['Survived'].sum()\n",
    "    print(f\"   • Survived: {survived_count} ({survived_count/len(first_class_clean)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n4. Sample of Cleaned Data:\")\n",
    "print(\"-\" * 40)\n",
    "# Show a few cleaned rows\n",
    "if len(first_class_clean) > 0:\n",
    "    sample = first_class_clean.head(3).copy()\n",
    "    # Format Age for display\n",
    "    sample['Age'] = sample['Age'].apply(lambda x: f\"{x:.1f}\")\n",
    "    if 'Fare' in sample.columns:\n",
    "        sample['Fare'] = sample['Fare'].apply(lambda x: f\"${x:.2f}\")\n",
    "    print(sample)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROBLEM 2 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nKey Answer:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of null values in 'Age' column: {null_count}\")\n",
    "print(f\"Rows dropped: {dropped_rows}\")\n",
    "print(f\"Final DataFrame shape after dropping: {first_class_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "441a3e5c-3776-4efc-9087-0aa5c55f011e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TITANIC DATASET ANALYSIS - PROBLEM 3\n",
      "ONE-HOT ENCODING OF 'EMBARKED' COLUMN\n",
      "================================================================================\n",
      "\n",
      "STEP 1: Loading Titanic dataset\n",
      "------------------------------------------------------------\n",
      "✓ 'titanic.csv' loaded successfully!\n",
      "\n",
      "Dataset Shape: (10, 12)\n",
      "Columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Analyzing 'Embarked' Column\n",
      "================================================================================\n",
      "\n",
      "1. Basic Information about 'Embarked' Column:\n",
      "------------------------------------------------------------\n",
      "Data type: object\n",
      "Unique values: ['S' 'C' 'Q']\n",
      "Number of unique values: 3\n",
      "\n",
      "2. Value Counts and Distribution:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Counts:\n",
      "  S (Southampton):   7 passengers (70.0%)\n",
      "  C (Cherbourg):   2 passengers (20.0%)\n",
      "  Q (Queenstown):   1 passengers (10.0%)\n",
      "\n",
      "3. Sample Rows with Embarked Information:\n",
      "------------------------------------------------------------\n",
      "   PassengerId                                               Name Embarked  \\\n",
      "0            1                            Braund, Mr. Owen Harris        S   \n",
      "1            2  Cumings, Mrs. John Bradley (Florence Briggs Th...        C   \n",
      "2            3                             Heikkinen, Miss. Laina        S   \n",
      "3            4       Futrelle, Mrs. Jacques Heath (Lily May Peel)        S   \n",
      "4            5                           Allen, Mr. William Henry        S   \n",
      "5            6                                   Moran, Mr. James        Q   \n",
      "6            7                            McCarthy, Mr. Timothy J        S   \n",
      "7            8                     Palsson, Master. Gosta Leonard        S   \n",
      "8            9  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)        S   \n",
      "9           10                Nasser, Mrs. Nicholas (Adele Achem)        C   \n",
      "\n",
      "     Port_Name  \n",
      "0  Southampton  \n",
      "1    Cherbourg  \n",
      "2  Southampton  \n",
      "3  Southampton  \n",
      "4  Southampton  \n",
      "5   Queenstown  \n",
      "6  Southampton  \n",
      "7  Southampton  \n",
      "8  Southampton  \n",
      "9    Cherbourg  \n",
      "\n",
      "4. Missing Values Check:\n",
      "------------------------------------------------------------\n",
      "✓ No missing values in 'Embarked' column\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PROBLEM 3: One-Hot Encoding\n",
      "================================================================================\n",
      "\n",
      "Task 1: Using one-hot encoding to convert 'Embarked' column\n",
      "------------------------------------------------------------\n",
      "Method: Using pandas.get_dummies()\n",
      "This method creates separate binary columns for each category\n",
      "\n",
      "Unique Embarked values to encode: ['C', 'Q', 'S']\n",
      "\n",
      "Performing one-hot encoding...\n",
      "\n",
      "Created 3 new binary columns:\n",
      "Columns: ['Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
      "Shape of encoded data: (10, 3)\n",
      "\n",
      "Sample of one-hot encoded data (first 10 rows):\n",
      "   Embarked_C  Embarked_Q  Embarked_S\n",
      "0       False       False        True\n",
      "1        True       False       False\n",
      "2       False       False        True\n",
      "3       False       False        True\n",
      "4       False       False        True\n",
      "5       False        True       False\n",
      "6       False       False        True\n",
      "7       False       False        True\n",
      "8       False       False        True\n",
      "9        True       False       False\n",
      "\n",
      "Task 2: Add these new columns to the original DataFrame\n",
      "------------------------------------------------------------\n",
      "Original DataFrame shape: (10, 12)\n",
      "Encoded DataFrame shape: (10, 15)\n",
      "Added 3 new columns\n",
      "\n",
      "Columns in the new DataFrame:\n",
      "Total columns: 15\n",
      "First 15 columns:\n",
      "   1. PassengerId\n",
      "   2. Survived\n",
      "   3. Pclass\n",
      "   4. Name\n",
      "   5. Sex\n",
      "   6. Age\n",
      "   7. SibSp\n",
      "   8. Parch\n",
      "   9. Ticket\n",
      "  10. Fare\n",
      "  11. Cabin\n",
      "  12. Embarked\n",
      "  13. Embarked_C\n",
      "  14. Embarked_Q\n",
      "  15. Embarked_S\n",
      "\n",
      "Verification of encoding (first 5 rows):\n",
      "   PassengerId                                               Name Embarked  \\\n",
      "0            1                            Braund, Mr. Owen Harris        S   \n",
      "1            2  Cumings, Mrs. John Bradley (Florence Briggs Th...        C   \n",
      "2            3                             Heikkinen, Miss. Laina        S   \n",
      "3            4       Futrelle, Mrs. Jacques Heath (Lily May Peel)        S   \n",
      "4            5                           Allen, Mr. William Henry        S   \n",
      "\n",
      "   Embarked_C  Embarked_Q  Embarked_S  \n",
      "0       False       False        True  \n",
      "1        True       False       False  \n",
      "2       False       False        True  \n",
      "3       False       False        True  \n",
      "4       False       False        True  \n",
      "\n",
      "Task 3: Drop the original 'Embarked' column\n",
      "------------------------------------------------------------\n",
      "✓ Dropped original 'Embarked' column\n",
      "DataFrame shape after dropping: (10, 14)\n",
      "Remaining columns: 14\n",
      "\n",
      "Original 'Embarked' column distribution (for reference):\n",
      "  S (Southampton): 7 passengers\n",
      "  C (Cherbourg): 2 passengers\n",
      "  Q (Queenstown): 1 passengers\n",
      "\n",
      "Task 4: Print the first few rows of the modified DataFrame\n",
      "------------------------------------------------------------\n",
      "\n",
      "First 10 rows of the modified DataFrame:\n",
      "------------------------------------------------------------\n",
      "   PassengerId                                               Name  Pclass  \\\n",
      "0            1                            Braund, Mr. Owen Harris       3   \n",
      "1            2  Cumings, Mrs. John Bradley (Florence Briggs Th...       1   \n",
      "2            3                             Heikkinen, Miss. Laina       3   \n",
      "3            4       Futrelle, Mrs. Jacques Heath (Lily May Peel)       1   \n",
      "4            5                           Allen, Mr. William Henry       3   \n",
      "5            6                                   Moran, Mr. James       3   \n",
      "6            7                            McCarthy, Mr. Timothy J       1   \n",
      "7            8                     Palsson, Master. Gosta Leonard       3   \n",
      "8            9  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)       3   \n",
      "9           10                Nasser, Mrs. Nicholas (Adele Achem)       2   \n",
      "\n",
      "      Sex   Age     Fare  Survived  Embarked_C  Embarked_Q  Embarked_S  \\\n",
      "0    male  22.0   7.2500         0       False       False        True   \n",
      "1  female  38.0  71.2833         1        True       False       False   \n",
      "2  female  26.0   7.9250         1       False       False        True   \n",
      "3  female  35.0  53.1000         1       False       False        True   \n",
      "4    male  35.0   8.0500         0       False       False        True   \n",
      "5    male   NaN   8.4583         0       False        True       False   \n",
      "6    male  54.0  51.8625         0       False       False        True   \n",
      "7    male   2.0  21.0750         0       False       False        True   \n",
      "8  female  27.0  11.1333         1       False       False        True   \n",
      "9  female  14.0  30.0708         1        True       False       False   \n",
      "\n",
      "  Port_Interpretation  \n",
      "0     S (Southampton)  \n",
      "1       C (Cherbourg)  \n",
      "2     S (Southampton)  \n",
      "3     S (Southampton)  \n",
      "4     S (Southampton)  \n",
      "5      Q (Queenstown)  \n",
      "6     S (Southampton)  \n",
      "7     S (Southampton)  \n",
      "8     S (Southampton)  \n",
      "9       C (Cherbourg)  \n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Verification and Analysis\n",
      "================================================================================\n",
      "\n",
      "1. Verification of One-Hot Encoding:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Checking encoding correctness:\n",
      "  Embarked_C: 2 ones (original had 2 'C' values)\n",
      "  Embarked_Q: 1 ones (original had 1 'Q' values)\n",
      "  Embarked_S: 7 ones (original had 7 'S' values)\n",
      "  Rows with no port selected: 0\n",
      "\n",
      "2. Statistical Analysis of Encoded Columns:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Summary statistics of one-hot encoded columns:\n",
      "  Embarked_C (Cherbourg):\n",
      "    Total passengers: 2\n",
      "    Percentage: 20.0%\n",
      "  Embarked_Q (Queenstown):\n",
      "    Total passengers: 1\n",
      "    Percentage: 10.0%\n",
      "  Embarked_S (Southampton):\n",
      "    Total passengers: 7\n",
      "    Percentage: 70.0%\n",
      "\n",
      "3. Relationship with Survival (if 'Survived' column exists):\n",
      "------------------------------------------------------------\n",
      "\n",
      "Survival rates by port of embarkation:\n",
      "  C (Cherbourg):\n",
      "    Total passengers: 2\n",
      "    Survived: 2\n",
      "    Survival rate: 100.0%\n",
      "  Q (Queenstown):\n",
      "    Total passengers: 1\n",
      "    Survived: 0\n",
      "    Survival rate: 0.0%\n",
      "  S (Southampton):\n",
      "    Total passengers: 7\n",
      "    Survived: 3\n",
      "    Survival rate: 42.9%\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 4: Manual Label Encoding Example\n",
      "================================================================================\n",
      "\n",
      "Manual Label Encoding (without scikit-learn):\n",
      "------------------------------------------------------------\n",
      "Port mapping used:\n",
      "  C (Cherbourg) → 0\n",
      "  Q (Queenstown) → 1\n",
      "  S (Southampton) → 2\n",
      "\n",
      "First 5 rows with Manual Label Encoding:\n",
      "   PassengerId Embarked  Embarked_Label\n",
      "0            1        S               2\n",
      "1            2        C               0\n",
      "2            3        S               2\n",
      "3            4        S               2\n",
      "4            5        S               2\n",
      "\n",
      "Comparison with One-Hot Encoding:\n",
      "------------------------------------------------------------\n",
      "One-Hot Encoding Advantages:\n",
      "  1. No implied ordinal relationship\n",
      "  2. Better for machine learning algorithms\n",
      "  3. Each category gets equal weight\n",
      "\n",
      "Label Encoding Disadvantages:\n",
      "  1. Implies C(0) < Q(1) < S(2) which is not true\n",
      "  2. Algorithms might misinterpret the numeric relationship\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY - PROBLEM 3\n",
      "================================================================================\n",
      "\n",
      "1. One-Hot Encoding Results:\n",
      "   • Original 'Embarked' column values: ['C', 'Q', 'S']\n",
      "   • Created 3 new binary columns:\n",
      "     - Embarked_C: 2 passengers from Cherbourg\n",
      "     - Embarked_Q: 1 passengers from Queenstown\n",
      "     - Embarked_S: 7 passengers from Southampton\n",
      "\n",
      "2. DataFrame Changes:\n",
      "   • Original shape: (10, 12)\n",
      "   • Final shape: (10, 14)\n",
      "   • Net column change: +2 columns\n",
      "\n",
      "3. Data Verification:\n",
      "   • Total rows processed: 10\n",
      "   • Rows with ambiguous encoding: 0\n",
      "\n",
      "4. Sample of Final Modified DataFrame:\n",
      "------------------------------------------------------------\n",
      "   PassengerId                                               Name  Embarked_C  \\\n",
      "0            1                            Braund, Mr. Owen Harris       False   \n",
      "1            2  Cumings, Mrs. John Bradley (Florence Briggs Th...        True   \n",
      "2            3                             Heikkinen, Miss. Laina       False   \n",
      "3            4       Futrelle, Mrs. Jacques Heath (Lily May Peel)       False   \n",
      "4            5                           Allen, Mr. William Henry       False   \n",
      "\n",
      "   Embarked_Q  Embarked_S  \n",
      "0       False        True  \n",
      "1       False       False  \n",
      "2       False        True  \n",
      "3       False        True  \n",
      "4       False        True  \n",
      "\n",
      "================================================================================\n",
      "PROBLEM 3 COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "All tasks completed:\n",
      "✓ 1. One-hot encoding applied to 'Embarked' column\n",
      "✓ 2. New columns added to DataFrame\n",
      "✓ 3. Original 'Embarked' column dropped\n",
      "✓ 4. Modified DataFrame displayed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TITANIC DATASET ANALYSIS - PROBLEM 3\")\n",
    "print(\"ONE-HOT ENCODING OF 'EMBARKED' COLUMN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load or create the Titanic dataset\n",
    "# ============================================================================\n",
    "print(\"\\nSTEP 1: Loading Titanic dataset\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check if titanic.csv exists\n",
    "if not os.path.exists('titanic.csv'):\n",
    "    print(\"Creating comprehensive Titanic dataset with Embarked data...\")\n",
    "    \n",
    "    # Create a realistic Titanic dataset with Embarked information\n",
    "    titanic_data = {\n",
    "        'PassengerId': list(range(1, 31)),\n",
    "        'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1] * 3,\n",
    "        'Pclass': [3, 1, 3, 1, 3, 3, 1, 3, 3, 2] * 3,\n",
    "        'Name': [f'Passenger {i}' for i in range(1, 31)],\n",
    "        'Sex': ['male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'female'] * 3,\n",
    "        'Age': [22, 38, 26, 35, 35, 20, 54, 2, 27, 14] * 3,\n",
    "        'SibSp': [1, 1, 0, 1, 0, 0, 0, 3, 0, 1] * 3,\n",
    "        'Parch': [0, 0, 0, 0, 0, 0, 0, 1, 2, 0] * 3,\n",
    "        'Ticket': [f'Ticket{i}' for i in range(1, 31)],\n",
    "        'Fare': [7.25, 71.28, 7.93, 53.10, 8.05, 8.46, 51.86, 21.08, 11.13, 30.07] * 3,\n",
    "        'Cabin': [None, 'C85', None, 'C123', None, None, 'E46', None, None, None] * 3,\n",
    "        'Embarked': ['S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'C'] * 3\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(titanic_data)\n",
    "    df.to_csv('titanic.csv', index=False)\n",
    "    print(\"✓ Sample 'titanic.csv' created with 30 records\")\n",
    "else:\n",
    "    # Load existing dataset\n",
    "    df = pd.read_csv('titanic.csv')\n",
    "    print(\"✓ 'titanic.csv' loaded successfully!\")\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Check the 'Embarked' column\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Analyzing 'Embarked' Column\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if 'Embarked' column exists\n",
    "if 'Embarked' not in df.columns:\n",
    "    print(\"✗ ERROR: 'Embarked' column not found in the dataset!\")\n",
    "    print(\"\\nAdding sample Embarked data...\")\n",
    "    \n",
    "    # Add Embarked column with sample data\n",
    "    embarked_values = ['S', 'C', 'Q', 'S', 'C', 'Q', 'S', 'C', 'Q', 'S']\n",
    "    df['Embarked'] = (embarked_values * (len(df) // len(embarked_values) + 1))[:len(df)]\n",
    "    print(\"✓ Added 'Embarked' column with sample data\")\n",
    "\n",
    "print(\"\\n1. Basic Information about 'Embarked' Column:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Data type: {df['Embarked'].dtype}\")\n",
    "print(f\"Unique values: {df['Embarked'].unique()}\")\n",
    "print(f\"Number of unique values: {df['Embarked'].nunique()}\")\n",
    "\n",
    "print(\"\\n2. Value Counts and Distribution:\")\n",
    "print(\"-\" * 60)\n",
    "embarked_counts = df['Embarked'].value_counts(dropna=False)\n",
    "embarked_percentage = df['Embarked'].value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "print(\"\\nCounts:\")\n",
    "for value, count in embarked_counts.items():\n",
    "    percentage = embarked_percentage[value]\n",
    "    port_name = {\n",
    "        'C': 'Cherbourg',\n",
    "        'Q': 'Queenstown', \n",
    "        'S': 'Southampton',\n",
    "        np.nan: 'Unknown/Missing'\n",
    "    }.get(value, str(value))\n",
    "    \n",
    "    print(f\"  {value} ({port_name}): {count:3d} passengers ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n3. Sample Rows with Embarked Information:\")\n",
    "print(\"-\" * 60)\n",
    "# Show first 10 rows with Embarked column\n",
    "sample_df = df[['PassengerId', 'Name', 'Embarked']].head(10).copy()\n",
    "# Add port names for better understanding\n",
    "def get_port_name(code):\n",
    "    port_names = {'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}\n",
    "    return port_names.get(str(code), 'Unknown')\n",
    "\n",
    "sample_df['Port_Name'] = sample_df['Embarked'].apply(get_port_name)\n",
    "print(sample_df)\n",
    "\n",
    "print(\"\\n4. Missing Values Check:\")\n",
    "print(\"-\" * 60)\n",
    "missing_embarked = df['Embarked'].isnull().sum()\n",
    "if missing_embarked > 0:\n",
    "    print(f\"⚠ Missing values in 'Embarked': {missing_embarked} ({missing_embarked/len(df)*100:.1f}%)\")\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    # For one-hot encoding, we'll keep missing as separate or fill with mode\n",
    "    mode_value = df['Embarked'].mode()[0] if not df['Embarked'].mode().empty else 'S'\n",
    "    print(f\"  Mode (most common) value: '{mode_value}'\")\n",
    "    print(f\"  Options: Fill with mode or keep as separate category\")\n",
    "else:\n",
    "    print(\"✓ No missing values in 'Embarked' column\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROBLEM 3: One-Hot Encoding of 'Embarked' Column\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"PROBLEM 3: One-Hot Encoding\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTask 1: Using one-hot encoding to convert 'Embarked' column\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Store original DataFrame for comparison\n",
    "df_original = df.copy()\n",
    "\n",
    "print(\"Method: Using pandas.get_dummies()\")\n",
    "print(\"This method creates separate binary columns for each category\")\n",
    "\n",
    "# Get unique values for reference\n",
    "unique_embarked = sorted([str(x) for x in df['Embarked'].unique() if pd.notna(x)])\n",
    "print(f\"\\nUnique Embarked values to encode: {unique_embarked}\")\n",
    "\n",
    "# Perform one-hot encoding\n",
    "print(\"\\nPerforming one-hot encoding...\")\n",
    "embarked_dummies = pd.get_dummies(df['Embarked'], prefix='Embarked')\n",
    "\n",
    "print(f\"\\nCreated {embarked_dummies.shape[1]} new binary columns:\")\n",
    "print(f\"Columns: {list(embarked_dummies.columns)}\")\n",
    "print(f\"Shape of encoded data: {embarked_dummies.shape}\")\n",
    "\n",
    "# Show sample of the encoded data\n",
    "print(\"\\nSample of one-hot encoded data (first 10 rows):\")\n",
    "print(embarked_dummies.head(10))\n",
    "\n",
    "print(\"\\nTask 2: Add these new columns to the original DataFrame\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Concatenate the original DataFrame with the dummy variables\n",
    "df_encoded = pd.concat([df, embarked_dummies], axis=1)\n",
    "\n",
    "print(f\"Original DataFrame shape: {df.shape}\")\n",
    "print(f\"Encoded DataFrame shape: {df_encoded.shape}\")\n",
    "print(f\"Added {embarked_dummies.shape[1]} new columns\")\n",
    "\n",
    "print(\"\\nColumns in the new DataFrame:\")\n",
    "print(f\"Total columns: {len(df_encoded.columns)}\")\n",
    "print(\"First 15 columns:\")\n",
    "for i, col in enumerate(df_encoded.columns[:15], 1):\n",
    "    print(f\"  {i:2}. {col}\")\n",
    "if len(df_encoded.columns) > 15:\n",
    "    print(f\"  ... and {len(df_encoded.columns) - 15} more columns\")\n",
    "\n",
    "# Verify the encoding\n",
    "print(\"\\nVerification of encoding (first 5 rows):\")\n",
    "verification_columns = ['PassengerId', 'Name', 'Embarked']\n",
    "for col in embarked_dummies.columns:\n",
    "    if col in df_encoded.columns:\n",
    "        verification_columns.append(col)\n",
    "        \n",
    "verification_df = df_encoded[verification_columns].head(5).copy()\n",
    "print(verification_df)\n",
    "\n",
    "print(\"\\nTask 3: Drop the original 'Embarked' column\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Store information about the column being dropped\n",
    "original_embarked_values = df_encoded['Embarked'].value_counts().to_dict()\n",
    "\n",
    "# Drop the original 'Embarked' column\n",
    "df_final = df_encoded.drop('Embarked', axis=1)\n",
    "\n",
    "print(f\"✓ Dropped original 'Embarked' column\")\n",
    "print(f\"DataFrame shape after dropping: {df_final.shape}\")\n",
    "print(f\"Remaining columns: {len(df_final.columns)}\")\n",
    "\n",
    "print(\"\\nOriginal 'Embarked' column distribution (for reference):\")\n",
    "for value, count in original_embarked_values.items():\n",
    "    port_name = {'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}.get(value, value)\n",
    "    print(f\"  {value} ({port_name}): {count} passengers\")\n",
    "\n",
    "print(\"\\nTask 4: Print the first few rows of the modified DataFrame\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nFirst 10 rows of the modified DataFrame:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Select relevant columns to display (including the new one-hot encoded columns)\n",
    "display_columns = ['PassengerId', 'Name', 'Pclass', 'Sex', 'Age', 'Fare', 'Survived']\n",
    "\n",
    "# Add the one-hot encoded columns\n",
    "one_hot_columns = [col for col in df_final.columns if col.startswith('Embarked_')]\n",
    "display_columns.extend(one_hot_columns)\n",
    "\n",
    "# Display the first 10 rows\n",
    "display_df = df_final[display_columns].head(10).copy()\n",
    "\n",
    "# Format for better readability\n",
    "def format_one_hot(row):\n",
    "    \"\"\"Helper function to show which port was selected\"\"\"\n",
    "    ports = []\n",
    "    for col in one_hot_columns:\n",
    "        if row[col] == 1:\n",
    "            port_code = col.replace('Embarked_', '')\n",
    "            port_name = {'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}.get(port_code, port_code)\n",
    "            ports.append(f\"{port_code} ({port_name})\")\n",
    "    return ', '.join(ports) if ports else 'None'\n",
    "\n",
    "# Add a column showing the port interpretation\n",
    "display_df['Port_Interpretation'] = display_df.apply(format_one_hot, axis=1)\n",
    "\n",
    "print(display_df)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Verification and Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: Verification and Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Verification of One-Hot Encoding:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check that each row has exactly one 1 in the one-hot encoded columns (for non-missing original values)\n",
    "print(\"\\nChecking encoding correctness:\")\n",
    "for port_code in ['C', 'Q', 'S']:\n",
    "    col_name = f'Embarked_{port_code}'\n",
    "    if col_name in df_final.columns:\n",
    "        count = df_final[col_name].sum()\n",
    "        original_count = original_embarked_values.get(port_code, 0)\n",
    "        print(f\"  {col_name}: {count} ones (original had {original_count} '{port_code}' values)\")\n",
    "\n",
    "# Check for rows where no port is selected (should be 0 for non-missing original data)\n",
    "if all(col in df_final.columns for col in ['Embarked_C', 'Embarked_Q', 'Embarked_S']):\n",
    "    no_port_selected = ((df_final['Embarked_C'] == 0) & \n",
    "                        (df_final['Embarked_Q'] == 0) & \n",
    "                        (df_final['Embarked_S'] == 0)).sum()\n",
    "    print(f\"  Rows with no port selected: {no_port_selected}\")\n",
    "\n",
    "print(\"\\n2. Statistical Analysis of Encoded Columns:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nSummary statistics of one-hot encoded columns:\")\n",
    "for col in one_hot_columns:\n",
    "    if col in df_final.columns:\n",
    "        col_sum = df_final[col].sum()\n",
    "        col_mean = df_final[col].mean() * 100\n",
    "        port_code = col.replace('Embarked_', '')\n",
    "        port_name = {'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}.get(port_code, port_code)\n",
    "        print(f\"  {col} ({port_name}):\")\n",
    "        print(f\"    Total passengers: {col_sum}\")\n",
    "        print(f\"    Percentage: {col_mean:.1f}%\")\n",
    "\n",
    "print(\"\\n3. Relationship with Survival (if 'Survived' column exists):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'Survived' in df_final.columns:\n",
    "    print(\"\\nSurvival rates by port of embarkation:\")\n",
    "    for col in one_hot_columns:\n",
    "        if col in df_final.columns:\n",
    "            port_code = col.replace('Embarked_', '')\n",
    "            port_name = {'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}.get(port_code, port_code)\n",
    "            \n",
    "            # Calculate survival rate for passengers from this port\n",
    "            mask = df_final[col] == 1\n",
    "            total_from_port = mask.sum()\n",
    "            if total_from_port > 0:\n",
    "                survived_from_port = df_final.loc[mask, 'Survived'].sum()\n",
    "                survival_rate = (survived_from_port / total_from_port) * 100\n",
    "                print(f\"  {port_code} ({port_name}):\")\n",
    "                print(f\"    Total passengers: {total_from_port}\")\n",
    "                print(f\"    Survived: {survived_from_port}\")\n",
    "                print(f\"    Survival rate: {survival_rate:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Manual Label Encoding Example (No scikit-learn needed)\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: Manual Label Encoding Example\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nManual Label Encoding (without scikit-learn):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create a manual label encoding\n",
    "df_manual = df_original.copy()\n",
    "\n",
    "# Define mapping manually\n",
    "port_mapping = {'C': 0, 'Q': 1, 'S': 2}\n",
    "\n",
    "# Apply manual label encoding\n",
    "df_manual['Embarked_Label'] = df_manual['Embarked'].map(port_mapping)\n",
    "\n",
    "print(\"Port mapping used:\")\n",
    "for port_code, label in port_mapping.items():\n",
    "    port_name = {'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}.get(port_code, port_code)\n",
    "    print(f\"  {port_code} ({port_name}) → {label}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows with Manual Label Encoding:\")\n",
    "print(df_manual[['PassengerId', 'Embarked', 'Embarked_Label']].head())\n",
    "\n",
    "print(\"\\nComparison with One-Hot Encoding:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"One-Hot Encoding Advantages:\")\n",
    "print(\"  1. No implied ordinal relationship\")\n",
    "print(\"  2. Better for machine learning algorithms\")\n",
    "print(\"  3. Each category gets equal weight\")\n",
    "\n",
    "print(\"\\nLabel Encoding Disadvantages:\")\n",
    "print(\"  1. Implies C(0) < Q(1) < S(2) which is not true\")\n",
    "print(\"  2. Algorithms might misinterpret the numeric relationship\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SUMMARY - PROBLEM 3\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. One-Hot Encoding Results:\")\n",
    "print(f\"   • Original 'Embarked' column values: {unique_embarked}\")\n",
    "print(f\"   • Created {len(one_hot_columns)} new binary columns:\")\n",
    "for col in one_hot_columns:\n",
    "    if col in df_final.columns:\n",
    "        port_code = col.replace('Embarked_', '')\n",
    "        port_name = {'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}.get(port_code, port_code)\n",
    "        count = df_final[col].sum()\n",
    "        print(f\"     - {col}: {count} passengers from {port_name}\")\n",
    "\n",
    "print(f\"\\n2. DataFrame Changes:\")\n",
    "print(f\"   • Original shape: {df.shape}\")\n",
    "print(f\"   • Final shape: {df_final.shape}\")\n",
    "print(f\"   • Net column change: +{len(df_final.columns) - len(df.columns)} columns\")\n",
    "\n",
    "print(f\"\\n3. Data Verification:\")\n",
    "print(f\"   • Total rows processed: {len(df_final)}\")\n",
    "if all(col in df_final.columns for col in ['Embarked_C', 'Embarked_Q', 'Embarked_S']):\n",
    "    no_port_selected = ((df_final['Embarked_C'] == 0) & \n",
    "                        (df_final['Embarked_Q'] == 0) & \n",
    "                        (df_final['Embarked_S'] == 0)).sum()\n",
    "    print(f\"   • Rows with ambiguous encoding: {no_port_selected}\")\n",
    "\n",
    "print(f\"\\n4. Sample of Final Modified DataFrame:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Show a compact view of the important columns\n",
    "sample_columns = ['PassengerId', 'Name']\n",
    "for col in one_hot_columns:\n",
    "    if col in df_final.columns:\n",
    "        sample_columns.append(col)\n",
    "\n",
    "sample_final = df_final[sample_columns].head(5).copy()\n",
    "print(sample_final)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROBLEM 3 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAll tasks completed:\")\n",
    "print(\"✓ 1. One-hot encoding applied to 'Embarked' column\")\n",
    "print(\"✓ 2. New columns added to DataFrame\")\n",
    "print(\"✓ 3. Original 'Embarked' column dropped\")\n",
    "print(\"✓ 4. Modified DataFrame displayed\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8061f3-24c1-40ec-8d90-a4ea0a327771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TITANIC DATASET ANALYSIS - PROBLEM 4\")\n",
    "print(\"SURVIVAL RATES BY GENDER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load or create the Titanic dataset\n",
    "# ============================================================================\n",
    "print(\"\\nSTEP 1: Loading Titanic dataset\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check if titanic.csv exists\n",
    "if not os.path.exists('titanic.csv'):\n",
    "    print(\"Creating Titanic dataset with survival data...\")\n",
    "    \n",
    "    # Create a realistic Titanic dataset with balanced survival data\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    titanic_data = {\n",
    "        'PassengerId': list(range(1, 101)),\n",
    "        'Survived': [],\n",
    "        'Pclass': [],\n",
    "        'Name': [f'Passenger {i}' for i in range(1, 101)],\n",
    "        'Sex': [],\n",
    "        'Age': [],\n",
    "        'Fare': [],\n",
    "        'Embarked': []\n",
    "    }\n",
    "    \n",
    "    # Generate realistic data with gender-based survival patterns\n",
    "    for i in range(100):\n",
    "        # Assign gender (approximately 65% male, 35% female like actual Titanic)\n",
    "        if np.random.random() < 0.65:\n",
    "            sex = 'male'\n",
    "            # Males had lower survival rate\n",
    "            survived = 1 if np.random.random() < 0.19 else 0  # ~19% survival for males\n",
    "            age = np.random.normal(30, 10)  # Average age around 30\n",
    "            fare = np.random.normal(25, 20)  # Average fare\n",
    "        else:\n",
    "            sex = 'female'\n",
    "            # Females had higher survival rate\n",
    "            survived = 1 if np.random.random() < 0.74 else 0  # ~74% survival for females\n",
    "            age = np.random.normal(28, 8)  # Slightly younger average\n",
    "            fare = np.random.normal(45, 30)  # Higher average fare\n",
    "        \n",
    "        # Assign passenger class\n",
    "        pclass = np.random.choice([1, 2, 3], p=[0.25, 0.25, 0.5])\n",
    "        \n",
    "        # Assign embarkation port\n",
    "        embarked = np.random.choice(['S', 'C', 'Q'], p=[0.72, 0.19, 0.09])\n",
    "        \n",
    "        titanic_data['Survived'].append(survived)\n",
    "        titanic_data['Pclass'].append(pclass)\n",
    "        titanic_data['Sex'].append(sex)\n",
    "        titanic_data['Age'].append(max(1, age))  # Ensure positive age\n",
    "        titanic_data['Fare'].append(max(0, fare))  # Ensure non-negative fare\n",
    "        titanic_data['Embarked'].append(embarked)\n",
    "    \n",
    "    df = pd.DataFrame(titanic_data)\n",
    "    df.to_csv('titanic.csv', index=False)\n",
    "    print(\"✓ Sample 'titanic.csv' created with 100 records\")\n",
    "    print(\"  Data includes realistic gender-based survival patterns\")\n",
    "else:\n",
    "    # Load existing dataset\n",
    "    df = pd.read_csv('titanic.csv')\n",
    "    print(\"✓ 'titanic.csv' loaded successfully!\")\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Check for required columns\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Checking for required columns\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "required_columns = ['Sex', 'Survived']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"✗ ERROR: Missing required columns: {missing_columns}\")\n",
    "    print(\"\\nAdding missing columns with sample data...\")\n",
    "    \n",
    "    if 'Sex' not in df.columns:\n",
    "        df['Sex'] = np.random.choice(['male', 'female'], size=len(df), p=[0.65, 0.35])\n",
    "        print(\"✓ Added 'Sex' column\")\n",
    "    \n",
    "    if 'Survived' not in df.columns:\n",
    "        # Create survival data with gender bias\n",
    "        df['Survived'] = 0\n",
    "        for idx, row in df.iterrows():\n",
    "            if 'Sex' in df.columns:\n",
    "                if row['Sex'] == 'female':\n",
    "                    df.loc[idx, 'Survived'] = 1 if np.random.random() < 0.74 else 0\n",
    "                else:\n",
    "                    df.loc[idx, 'Survived'] = 1 if np.random.random() < 0.19 else 0\n",
    "        print(\"✓ Added 'Survived' column with gender-based survival patterns\")\n",
    "else:\n",
    "    print(\"✓ All required columns present!\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROBLEM 4: Compare mean survival rates by gender\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"PROBLEM 4: Compare Mean Survival Rates by Gender\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTask 1: Calculate mean survival rates for different gender groups\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n1. Basic Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Total counts\n",
    "total_passengers = len(df)\n",
    "print(f\"Total passengers: {total_passengers}\")\n",
    "\n",
    "# Gender distribution\n",
    "gender_counts = df['Sex'].value_counts()\n",
    "gender_percentage = df['Sex'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nGender Distribution:\")\n",
    "for gender, count in gender_counts.items():\n",
    "    percentage = gender_percentage[gender]\n",
    "    print(f\"  {gender.capitalize()}: {count} passengers ({percentage:.1f}%)\")\n",
    "\n",
    "# Survival distribution\n",
    "survival_counts = df['Survived'].value_counts()\n",
    "survival_percentage = df['Survived'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nOverall Survival:\")\n",
    "print(f\"  Survived: {survival_counts.get(1, 0)} passengers ({survival_percentage.get(1, 0):.1f}%)\")\n",
    "print(f\"  Did not survive: {survival_counts.get(0, 0)} passengers ({survival_percentage.get(0, 0):.1f}%)\")\n",
    "\n",
    "print(\"\\n2. Mean Survival Rates by Gender:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate mean survival rate by gender\n",
    "survival_by_gender = df.groupby('Sex')['Survived'].agg(['mean', 'sum', 'count']).reset_index()\n",
    "survival_by_gender.columns = ['Gender', 'Mean_Survival_Rate', 'Survived_Count', 'Total_Count']\n",
    "survival_by_gender['Mean_Survival_Rate'] = survival_by_gender['Mean_Survival_Rate'].round(3)\n",
    "survival_by_gender['Survival_Percentage'] = (survival_by_gender['Mean_Survival_Rate'] * 100).round(1)\n",
    "\n",
    "print(\"\\nDetailed Statistics:\")\n",
    "print(survival_by_gender.to_string(index=False))\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\n3. Additional Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for _, row in survival_by_gender.iterrows():\n",
    "    gender = row['Gender']\n",
    "    survived = row['Survived_Count']\n",
    "    total = row['Total_Count']\n",
    "    percentage = row['Survival_Percentage']\n",
    "    \n",
    "    print(f\"\\n{gender.capitalize()} passengers:\")\n",
    "    print(f\"  Total: {total}\")\n",
    "    print(f\"  Survived: {survived}\")\n",
    "    print(f\"  Survival rate: {percentage}%\")\n",
    "    print(f\"  Did not survive: {total - survived} ({(100 - percentage):.1f}%)\")\n",
    "\n",
    "# Calculate difference in survival rates\n",
    "if len(survival_by_gender) == 2:\n",
    "    female_rate = survival_by_gender[survival_by_gender['Gender'] == 'female']['Mean_Survival_Rate'].values[0]\n",
    "    male_rate = survival_by_gender[survival_by_gender['Gender'] == 'male']['Mean_Survival_Rate'].values[0]\n",
    "    difference = (female_rate - male_rate) * 100\n",
    "    \n",
    "    print(f\"\\n4. Gender Survival Gap:\")\n",
    "    print(f\"  Female survival rate: {female_rate*100:.1f}%\")\n",
    "    print(f\"  Male survival rate: {male_rate*100:.1f}%\")\n",
    "    print(f\"  Difference: {difference:.1f} percentage points\")\n",
    "    print(f\"  Females were {female_rate/male_rate:.1f}x more likely to survive than males\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 2: Create visualizations\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"TASK 2: Visualizing Survival Distributions by Gender\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nCreating visualizations...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Titanic Survival Analysis by Gender', fontsize=16, fontweight='bold')\n",
    "\n",
    "# ========== Plot 1: Bar Chart - Survival Rates by Gender ==========\n",
    "ax1 = axes[0, 0]\n",
    "gender_order = ['female', 'male']\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "bar_positions = np.arange(len(gender_order))\n",
    "\n",
    "survival_rates = [survival_by_gender[survival_by_gender['Gender'] == g]['Mean_Survival_Rate'].values[0] \n",
    "                  for g in gender_order]\n",
    "\n",
    "bars = ax1.bar(bar_positions, survival_rates, color=colors, edgecolor='black', width=0.6)\n",
    "\n",
    "ax1.set_title('Survival Rates by Gender', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Gender', fontsize=12)\n",
    "ax1.set_ylabel('Survival Rate', fontsize=12)\n",
    "ax1.set_xticks(bar_positions)\n",
    "ax1.set_xticklabels(['Female', 'Male'])\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar, rate in zip(bars, survival_rates):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{rate:.1%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# ========== Plot 2: Stacked Bar Chart - Counts by Gender and Survival ==========\n",
    "ax2 = axes[0, 1]\n",
    "\n",
    "# Prepare data for stacked bar\n",
    "survival_counts_by_gender = df.groupby(['Sex', 'Survived']).size().unstack(fill_value=0)\n",
    "survival_counts_by_gender = survival_counts_by_gender.reindex(gender_order)\n",
    "\n",
    "# Colors for survived (1) and not survived (0)\n",
    "stack_colors = ['#ff9999', '#66b3ff']\n",
    "\n",
    "bars_not_survived = ax2.bar(bar_positions, survival_counts_by_gender[0], \n",
    "                            color=stack_colors[0], edgecolor='black', width=0.6, label='Did Not Survive')\n",
    "bars_survived = ax2.bar(bar_positions, survival_counts_by_gender[1], \n",
    "                        bottom=survival_counts_by_gender[0], color=stack_colors[1], \n",
    "                        edgecolor='black', width=0.6, label='Survived')\n",
    "\n",
    "ax2.set_title('Passenger Counts by Gender and Survival', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Gender', fontsize=12)\n",
    "ax2.set_ylabel('Number of Passengers', fontsize=12)\n",
    "ax2.set_xticks(bar_positions)\n",
    "ax2.set_xticklabels(['Female', 'Male'])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add total count labels\n",
    "for i, (not_survived, survived) in enumerate(zip(survival_counts_by_gender[0], survival_counts_by_gender[1])):\n",
    "    total = not_survived + survived\n",
    "    ax2.text(i, total + max(total*0.02, 1), f'Total: {total}', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# ========== Plot 3: Pie Chart - Gender Distribution of Survivors ==========\n",
    "ax3 = axes[0, 2]\n",
    "\n",
    "# Calculate survivors by gender\n",
    "survivors_by_gender = df[df['Survived'] == 1]['Sex'].value_counts()\n",
    "if not survivors_by_gender.empty:\n",
    "    pie_colors = ['lightcoral', 'lightblue']\n",
    "    wedges, texts, autotexts = ax3.pie(survivors_by_gender.values, labels=[f'{g.capitalize()}' for g in survivors_by_gender.index],\n",
    "                                       autopct='%1.1f%%', colors=pie_colors, startangle=90)\n",
    "    \n",
    "    ax3.set_title('Gender Distribution Among Survivors', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Make percentages bold\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontweight('bold')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No survivors in dataset', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Gender Distribution Among Survivors', fontsize=14, fontweight='bold')\n",
    "\n",
    "# ========== Plot 4: Donut Chart - Survival Rate Comparison ==========\n",
    "ax4 = axes[1, 0]\n",
    "\n",
    "# Create donut chart for overall survival vs gender-specific\n",
    "categories = ['Overall Survival', 'Female Survival', 'Male Survival']\n",
    "values = [df['Survived'].mean(), \n",
    "          df[df['Sex'] == 'female']['Survived'].mean(),\n",
    "          df[df['Sex'] == 'male']['Survived'].mean()]\n",
    "colors_donut = ['lightgray', 'lightcoral', 'lightblue']\n",
    "\n",
    "# Outer ring: categories\n",
    "wedges1, texts1, autotexts1 = ax4.pie(values, labels=categories, colors=colors_donut,\n",
    "                                      autopct=lambda pct: f'{pct:.1f}%',\n",
    "                                      pctdistance=0.85, startangle=90)\n",
    "\n",
    "# Draw a circle in the center to make it a donut\n",
    "centre_circle = plt.Circle((0,0), 0.70, fc='white')\n",
    "ax4.add_artist(centre_circle)\n",
    "\n",
    "ax4.set_title('Survival Rate Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.text(0, 0, 'Survival\\nRates', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# ========== Plot 5: Heatmap-style Visualization ==========\n",
    "ax5 = axes[1, 1]\n",
    "\n",
    "# Create a 2x2 matrix for gender vs survival\n",
    "matrix_data = pd.crosstab(df['Sex'], df['Survived'], normalize='index')\n",
    "matrix_data = matrix_data.reindex(gender_order)\n",
    "\n",
    "# Create heatmap\n",
    "im = ax5.imshow(matrix_data.values, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(matrix_data.shape[0]):\n",
    "    for j in range(matrix_data.shape[1]):\n",
    "        value = matrix_data.iloc[i, j]\n",
    "        text_color = 'white' if value > 0.5 else 'black'\n",
    "        ax5.text(j, i, f'{value:.1%}', ha='center', va='center', \n",
    "                 color=text_color, fontsize=12, fontweight='bold')\n",
    "\n",
    "ax5.set_title('Survival Probability by Gender', fontsize=14, fontweight='bold')\n",
    "ax5.set_xlabel('Survived', fontsize=12)\n",
    "ax5.set_ylabel('Gender', fontsize=12)\n",
    "ax5.set_xticks([0, 1])\n",
    "ax5.set_xticklabels(['No (0)', 'Yes (1)'])\n",
    "ax5.set_yticks([0, 1])\n",
    "ax5.set_yticklabels(['Female', 'Male'])\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax5, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Probability', fontsize=10)\n",
    "\n",
    "# ========== Plot 6: Side-by-side Comparison ==========\n",
    "ax6 = axes[1, 2]\n",
    "\n",
    "# Create side-by-side bars for each gender showing survived vs not survived as percentages\n",
    "gender_labels = ['Female', 'Male']\n",
    "x_pos = np.arange(len(gender_labels))\n",
    "width = 0.35\n",
    "\n",
    "# Calculate percentages\n",
    "not_survived_pct = [(survival_counts_by_gender.loc[g, 0] / survival_counts_by_gender.loc[g].sum()) * 100 \n",
    "                    for g in gender_order]\n",
    "survived_pct = [(survival_counts_by_gender.loc[g, 1] / survival_counts_by_gender.loc[g].sum()) * 100 \n",
    "                for g in gender_order]\n",
    "\n",
    "bars1 = ax6.bar(x_pos - width/2, not_survived_pct, width, label='Did Not Survive', color='#ff9999', edgecolor='black')\n",
    "bars2 = ax6.bar(x_pos + width/2, survived_pct, width, label='Survived', color='#66b3ff', edgecolor='black')\n",
    "\n",
    "ax6.set_title('Survival Distribution by Gender (%)', fontsize=14, fontweight='bold')\n",
    "ax6.set_xlabel('Gender', fontsize=12)\n",
    "ax6.set_ylabel('Percentage', fontsize=12)\n",
    "ax6.set_xticks(x_pos)\n",
    "ax6.set_xticklabels(gender_labels)\n",
    "ax6.set_ylim(0, 100)\n",
    "ax6.legend()\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                 f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "\n",
    "print(\"✓ Created 6 different visualizations\")\n",
    "print(\"\\nVisualization Types Created:\")\n",
    "print(\"  1. Bar Chart - Survival Rates by Gender\")\n",
    "print(\"  2. Stacked Bar - Passenger Counts\")\n",
    "print(\"  3. Pie Chart - Gender Distribution of Survivors\")\n",
    "print(\"  4. Donut Chart - Survival Rate Comparison\")\n",
    "print(\"  5. Heatmap - Survival Probability Matrix\")\n",
    "print(\"  6. Side-by-side Bars - Percentage Distribution\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Statistical Test (Bonus)\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"BONUS: Statistical Significance Test\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nChecking if the difference in survival rates is statistically significant...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = pd.crosstab(df['Sex'], df['Survived'])\n",
    "print(\"\\nContingency Table:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Calculate expected values if there was no relationship\n",
    "total = contingency_table.sum().sum()\n",
    "row_totals = contingency_table.sum(axis=1)\n",
    "col_totals = contingency_table.sum(axis=0)\n",
    "\n",
    "expected = pd.DataFrame()\n",
    "for i in contingency_table.index:\n",
    "    for j in contingency_table.columns:\n",
    "        expected.loc[i, j] = (row_totals[i] * col_totals[j]) / total\n",
    "\n",
    "print(\"\\nExpected Values (if no relationship):\")\n",
    "print(expected)\n",
    "\n",
    "# Manual chi-square calculation for educational purposes\n",
    "chi_square = 0\n",
    "for i in contingency_table.index:\n",
    "    for j in contingency_table.columns:\n",
    "        observed = contingency_table.loc[i, j]\n",
    "        exp = expected.loc[i, j]\n",
    "        chi_square += (observed - exp) ** 2 / exp\n",
    "\n",
    "print(f\"\\nChi-square statistic: {chi_square:.4f}\")\n",
    "\n",
    "# Degrees of freedom\n",
    "df_chi = (contingency_table.shape[0] - 1) * (contingency_table.shape[1] - 1)\n",
    "print(f\"Degrees of freedom: {df_chi}\")\n",
    "\n",
    "# Critical value for alpha=0.05 (simplified interpretation)\n",
    "critical_value_95 = 3.841  # For df=1\n",
    "print(f\"Critical value (95% confidence): {critical_value_95}\")\n",
    "\n",
    "if chi_square > critical_value_95:\n",
    "    print(\"✓ The difference in survival rates between genders is statistically significant (p < 0.05)\")\n",
    "    print(\"  We reject the null hypothesis that survival is independent of gender.\")\n",
    "else:\n",
    "    print(\"✗ The difference is not statistically significant\")\n",
    "    print(\"  We cannot reject the null hypothesis that survival is independent of gender.\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SUMMARY - PROBLEM 4\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Key Findings:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for _, row in survival_by_gender.iterrows():\n",
    "    gender = row['Gender'].capitalize()\n",
    "    rate = row['Mean_Survival_Rate']\n",
    "    percentage = row['Survival_Percentage']\n",
    "    total = row['Total_Count']\n",
    "    survived = row['Survived_Count']\n",
    "    \n",
    "    print(f\"\\n{gender}:\")\n",
    "    print(f\"  • Survival rate: {percentage}%\")\n",
    "    print(f\"  • {survived} out of {total} passengers survived\")\n",
    "    print(f\"  • {total - survived} passengers did not survive\")\n",
    "\n",
    "if len(survival_by_gender) == 2:\n",
    "    female_data = survival_by_gender[survival_by_gender['Gender'] == 'female'].iloc[0]\n",
    "    male_data = survival_by_gender[survival_by_gender['Gender'] == 'male'].iloc[0]\n",
    "    \n",
    "    difference = female_data['Survival_Percentage'] - male_data['Survival_Percentage']\n",
    "    ratio = female_data['Mean_Survival_Rate'] / male_data['Mean_Survival_Rate']\n",
    "    \n",
    "    print(f\"\\nGender Survival Gap:\")\n",
    "    print(f\"  • Difference: {difference:.1f} percentage points\")\n",
    "    print(f\"  • Females were {ratio:.1f}x more likely to survive than males\")\n",
    "    print(f\"  • Statistical significance: {'Yes' if chi_square > critical_value_95 else 'No'}\")\n",
    "\n",
    "print(\"\\n2. Visualization Insights:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  • Visualizations clearly show the 'women and children first' policy\")\n",
    "print(\"  • Female survival rate is significantly higher than male survival rate\")\n",
    "print(\"  • The gender gap in survival is visually evident in all charts\")\n",
    "\n",
    "print(\"\\n3. Historical Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  • On the Titanic, survival rates were:\")\n",
    "print(\"    - Women: ~74%\")\n",
    "print(\"    - Men: ~19%\")\n",
    "print(\"    - Children: ~50%\")\n",
    "print(\"  • The 'women and children first' protocol was followed during evacuation\")\n",
    "\n",
    "# Save the visualizations\n",
    "output_file = 'titanic_survival_by_gender.png'\n",
    "plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\n✓ Visualizations saved as '{output_file}'\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROBLEM 4 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d2ab5-a869-46e8-bb33-42c1a0374ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
