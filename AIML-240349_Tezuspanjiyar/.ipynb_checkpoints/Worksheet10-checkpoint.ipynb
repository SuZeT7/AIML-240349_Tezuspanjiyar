{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for Exercise 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXERCISE 1: IMDB SENTIMENT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(\"✓ NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "df_imdb = pd.read_csv('IMDB_Dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Shape: {df_imdb.shape}\")\n",
    "print(f\"\\nColumn Names: {df_imdb.columns.tolist()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(df_imdb.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 60)\n",
    "print(df_imdb.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(\"=\" * 60)\n",
    "print(df_imdb.isnull().sum())\n",
    "\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "print(df_imdb['sentiment'].value_counts())\n",
    "print(f\"\\nPercentage Distribution:\")\n",
    "print(df_imdb['sentiment'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df_imdb, x='sentiment', palette='Set2', ax=axes[0])\n",
    "axes[0].set_title('Sentiment Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container)\n",
    "\n",
    "# Pie chart\n",
    "sentiment_counts = df_imdb['sentiment'].value_counts()\n",
    "axes[1].pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "            startangle=90, colors=['#ff9999', '#66b3ff'])\n",
    "axes[1].set_title('Sentiment Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text according to requirements:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove non-alphabetic characters (punctuation)\n",
    "    3. Tokenize and remove stopwords\n",
    "    4. Apply stemming\n",
    "    \"\"\"\n",
    "    # Step 1: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Remove non-alphabetic characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Step 3: Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Step 4: Apply stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Preprocessing function created successfully!\")\n",
    "print(\"\\nExample preprocessing:\")\n",
    "sample_text = \"This movie was AMAZING! I loved it so much. Best film ever!!!\"\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Processed: {preprocess_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all reviews\n",
    "print(\"Preprocessing all reviews...\")\n",
    "df_imdb['processed_review'] = df_imdb['review'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\n✓ Preprocessing completed!\")\n",
    "print(\"\\nComparison of Original vs Processed Reviews:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(2):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original ({len(df_imdb['review'].iloc[i])} chars):\")\n",
    "    print(df_imdb['review'].iloc[i][:200] + \"...\")\n",
    "    print(f\"\\nProcessed ({len(df_imdb['processed_review'].iloc[i])} chars):\")\n",
    "    print(df_imdb['processed_review'].iloc[i][:200] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split Dataset (80% Training, 20% Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X_imdb = df_imdb['processed_review']\n",
    "y_imdb = df_imdb['sentiment'].map({'negative': 0, 'positive': 1})\n",
    "\n",
    "# Split dataset: 80% training, 20% testing\n",
    "X_train_imdb, X_test_imdb, y_train_imdb, y_test_imdb = train_test_split(\n",
    "    X_imdb, y_imdb, test_size=0.2, random_state=42, stratify=y_imdb\n",
    ")\n",
    "\n",
    "print(\"Dataset Split Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(df_imdb)}\")\n",
    "print(f\"Training samples: {len(X_train_imdb)} ({len(X_train_imdb)/len(df_imdb)*100:.1f}%)\")\n",
    "print(f\"Testing samples: {len(X_test_imdb)} ({len(X_test_imdb)/len(df_imdb)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTraining Set Sentiment Distribution:\")\n",
    "print(y_train_imdb.value_counts())\n",
    "print(f\"\\nTesting Set Sentiment Distribution:\")\n",
    "print(y_test_imdb.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Bag-of-Words Model and Train Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a: Implement Bag-of-Words model using CountVectorizer\n",
    "print(\"Creating Bag-of-Words model...\")\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_bow = vectorizer.fit_transform(X_train_imdb)\n",
    "\n",
    "# Transform test data\n",
    "X_test_bow = vectorizer.transform(X_test_imdb)\n",
    "\n",
    "print(\"\\n✓ Bag-of-Words model created successfully!\")\n",
    "print(\"\\nBag-of-Words Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"Training matrix shape: {X_train_bow.shape}\")\n",
    "print(f\"Testing matrix shape: {X_test_bow.shape}\")\n",
    "print(f\"Matrix density: {X_train_bow.nnz / (X_train_bow.shape[0] * X_train_bow.shape[1]) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b: Train Naive Bayes classifier\n",
    "print(\"Training Naive Bayes classifier...\")\n",
    "\n",
    "# Initialize and train the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_bow, y_train_imdb)\n",
    "\n",
    "print(\"\\n✓ Naive Bayes classifier trained successfully!\")\n",
    "print(\"\\nModel Parameters:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of classes: {len(nb_classifier.classes_)}\")\n",
    "print(f\"Classes: {nb_classifier.classes_}\")\n",
    "print(f\"Number of features: {nb_classifier.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred_nb = nb_classifier.predict(X_test_bow)\n",
    "y_pred_proba_nb = nb_classifier.predict_proba(X_test_bow)[:, 1]\n",
    "\n",
    "print(\"✓ Predictions completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric 1: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_nb = accuracy_score(y_test_imdb, y_pred_nb)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ACCURACY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy Score: {accuracy_nb:.4f} ({accuracy_nb*100:.2f}%)\")\n",
    "print(f\"\\nCorrectly classified: {(y_test_imdb == y_pred_nb).sum()} out of {len(y_test_imdb)}\")\n",
    "print(f\"Misclassified: {(y_test_imdb != y_pred_nb).sum()} out of {len(y_test_imdb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric 2: Precision, Recall, and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision, recall, and F1-score\n",
    "precision_nb = precision_score(y_test_imdb, y_pred_nb)\n",
    "recall_nb = recall_score(y_test_imdb, y_pred_nb)\n",
    "f1_nb = f1_score(y_test_imdb, y_pred_nb)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRECISION, RECALL, AND F1-SCORE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision: {precision_nb:.4f}\")\n",
    "print(f\"Recall:    {recall_nb:.4f}\")\n",
    "print(f\"F1-Score:  {f1_nb:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test_imdb, y_pred_nb, \n",
    "                          target_names=['Negative (0)', 'Positive (1)'],\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric 3: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm_nb = confusion_matrix(y_test_imdb, y_pred_nb)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 60)\n",
    "print(cm_nb)\n",
    "print(\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"True Negatives (TN):  {cm_nb[0, 0]}\")\n",
    "print(f\"False Positives (FP): {cm_nb[0, 1]}\")\n",
    "print(f\"False Negatives (FN): {cm_nb[1, 0]}\")\n",
    "print(f\"True Positives (TP):  {cm_nb[1, 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative (0)', 'Positive (1)'],\n",
    "            yticklabels=['Negative (0)', 'Positive (1)'],\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            annot_kws={'size': 16, 'weight': 'bold'})\n",
    "\n",
    "plt.title('Confusion Matrix - Naive Bayes Classifier', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Actual Sentiment', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Predicted Sentiment', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric 4: ROC-AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC-AUC score\n",
    "roc_auc_nb = roc_auc_score(y_test_imdb, y_pred_proba_nb)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ROC-AUC SCORE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ROC-AUC Score: {roc_auc_nb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_imdb, y_pred_proba_nb)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc_nb:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "         label='Random Classifier (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "plt.title('ROC Curve - Naive Bayes Classifier', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary_ex1 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Score': [accuracy_nb, precision_nb, recall_nb, f1_nb, roc_auc_nb]\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 1 - FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(summary_ex1.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"• The Naive Bayes model achieved {accuracy_nb*100:.2f}% accuracy\")\n",
    "print(f\"• Precision: {precision_nb*100:.2f}% of positive predictions were correct\")\n",
    "print(f\"• Recall: {recall_nb*100:.2f}% of positive reviews were identified\")\n",
    "print(f\"• ROC-AUC: {roc_auc_nb:.4f} indicates strong discrimination ability\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Exercise 3: Feature Selection using Wrapper Methods\n",
    "\n",
    "## Objective\n",
    "Identify the most important features in predicting breast cancer prognosis using the Breast Cancer Dataset. Apply Recursive Feature Elimination (RFE) wrapper method to select the best features and evaluate model performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for Exercise 3\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXERCISE 3: FEATURE SELECTION WITH RFE\")\n",
    "print(\"=\"*70)\n",
    "print(\"✓ Additional libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "df_cancer = pd.read_csv('Breast_Cancer_Dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Shape: {df_cancer.shape}\")\n",
    "print(f\"Number of samples: {df_cancer.shape[0]}\")\n",
    "print(f\"Number of features: {df_cancer.shape[1] - 1}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(df_cancer.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_cancer.info())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Dataset Shape:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Rows: {df_cancer.shape[0]}\")\n",
    "print(f\"Columns: {df_cancer.shape[1]}\")\n",
    "print(f\"Features (excluding target): {df_cancer.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "missing_values = df_cancer.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"✓ No missing values found in the dataset!\")\n",
    "else:\n",
    "    print(\"Missing values per column:\")\n",
    "    print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "print(\"Target Variable Analysis (Diagnosis):\")\n",
    "print(\"=\" * 70)\n",
    "print(df_cancer['diagnosis'].value_counts())\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"Malignant (0): {(df_cancer['diagnosis'] == 0).sum()} ({(df_cancer['diagnosis'] == 0).sum()/len(df_cancer)*100:.2f}%)\")\n",
    "print(f\"Benign (1):    {(df_cancer['diagnosis'] == 1).sum()} ({(df_cancer['diagnosis'] == 1).sum()/len(df_cancer)*100:.2f}%)\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "target_counts = df_cancer['diagnosis'].value_counts()\n",
    "axes[0].bar(['Malignant (0)', 'Benign (1)'], target_counts.values, \n",
    "            color=['#ff6b6b', '#4ecdc4'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Diagnosis Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "axes[1].pie(target_counts.values, labels=['Malignant (0)', 'Benign (1)'],\n",
    "            autopct='%1.1f%%', startangle=90, colors=['#ff6b6b', '#4ecdc4'])\n",
    "axes[1].set_title('Diagnosis Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of features\n",
    "print(\"Statistical Summary of Features:\")\n",
    "print(\"=\" * 70)\n",
    "display(df_cancer.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split Dataset (80% Training, 20% Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_cancer = df_cancer.drop('diagnosis', axis=1)\n",
    "y_cancer = df_cancer['diagnosis']\n",
    "\n",
    "# Split dataset: 80% training, 20% testing\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "print(\"Dataset Split Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total samples: {len(df_cancer)}\")\n",
    "print(f\"Number of features: {X_cancer.shape[1]}\")\n",
    "print(f\"\\nTraining samples: {len(X_train_cancer)} ({len(X_train_cancer)/len(df_cancer)*100:.1f}%)\")\n",
    "print(f\"Testing samples:  {len(X_test_cancer)} ({len(X_test_cancer)/len(df_cancer)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTraining Set Class Distribution:\")\n",
    "print(f\"Malignant (0): {(y_train_cancer == 0).sum()} ({(y_train_cancer == 0).sum()/len(y_train_cancer)*100:.1f}%)\")\n",
    "print(f\"Benign (1):    {(y_train_cancer == 1).sum()} ({(y_train_cancer == 1).sum()/len(y_train_cancer)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (Important for Logistic Regression)\n",
    "print(\"Applying Feature Scaling...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_cancer_scaled = scaler.fit_transform(X_train_cancer)\n",
    "X_test_cancer_scaled = scaler.transform(X_test_cancer)\n",
    "\n",
    "print(\"✓ Features scaled successfully using StandardScaler!\")\n",
    "print(f\"\\nScaled training data shape: {X_train_cancer_scaled.shape}\")\n",
    "print(f\"Scaled testing data shape:  {X_test_cancer_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Apply Wrapper Method (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Recursive Feature Elimination (RFE) - Select Top 5 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression model for RFE\n",
    "print(\"Applying Recursive Feature Elimination (RFE)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create base estimator\n",
    "base_estimator = LogisticRegression(max_iter=10000, random_state=42)\n",
    "\n",
    "# Create RFE object to select top 5 features\n",
    "n_features_to_select = 5\n",
    "rfe = RFE(estimator=base_estimator, n_features_to_select=n_features_to_select)\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "\n",
    "print(f\"✓ RFE completed successfully!\")\n",
    "print(f\"\\nNumber of features selected: {n_features_to_select}\")\n",
    "print(f\"Total features: {X_train_cancer.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature rankings and selected features\n",
    "feature_names = X_cancer.columns.tolist()\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Ranking': rfe.ranking_,\n",
    "    'Selected': rfe.support_\n",
    "}).sort_values('Ranking')\n",
    "\n",
    "print(\"Feature Ranking by RFE:\")\n",
    "print(\"=\" * 70)\n",
    "print(feature_ranking.to_string(index=False))\n",
    "\n",
    "selected_features = feature_ranking[feature_ranking['Selected'] == True]['Feature'].tolist()\n",
    "print(f\"\\n✓ Top {n_features_to_select} Selected Features:\")\n",
    "print(\"=\" * 70)\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Visualize Feature Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature rankings\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create color map\n",
    "colors = ['green' if selected else 'red' for selected in feature_ranking['Selected']]\n",
    "\n",
    "plt.barh(feature_ranking['Feature'], feature_ranking['Ranking'], color=colors, alpha=0.7)\n",
    "plt.xlabel('Ranking (1 = Most Important)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
    "plt.title('RFE Feature Ranking', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='green', alpha=0.7, label='Selected'),\n",
    "                   Patch(facecolor='red', alpha=0.7, label='Not Selected')]\n",
    "plt.legend(handles=legend_elements, loc='lower right', fontsize=11)\n",
    "\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train Model with Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract selected features\n",
    "X_train_selected = X_train_cancer_scaled[:, rfe.support_]\n",
    "X_test_selected = X_test_cancer_scaled[:, rfe.support_]\n",
    "\n",
    "print(f\"Training model with {n_features_to_select} selected features...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Selected features shape (train): {X_train_selected.shape}\")\n",
    "print(f\"Selected features shape (test):  {X_test_selected.shape}\")\n",
    "\n",
    "# Train Logistic Regression with selected features\n",
    "lr_selected = LogisticRegression(max_iter=10000, random_state=42)\n",
    "lr_selected.fit(X_train_selected, y_train_cancer)\n",
    "\n",
    "print(\"\\n✓ Model trained successfully with selected features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with selected features model\n",
    "y_pred_selected = lr_selected.predict(X_test_selected)\n",
    "y_pred_proba_selected = lr_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Calculate metrics for selected features\n",
    "acc_selected = accuracy_score(y_test_cancer, y_pred_selected)\n",
    "prec_selected = precision_score(y_test_cancer, y_pred_selected)\n",
    "rec_selected = recall_score(y_test_cancer, y_pred_selected)\n",
    "f1_selected = f1_score(y_test_cancer, y_pred_selected)\n",
    "roc_selected = roc_auc_score(y_test_cancer, y_pred_proba_selected)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MODEL EVALUATION - SELECTED FEATURES ({n_features_to_select})\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy:  {acc_selected:.4f}\")\n",
    "print(f\"Precision: {prec_selected:.4f}\")\n",
    "print(f\"Recall:    {rec_selected:.4f}\")\n",
    "print(f\"F1-Score:  {f1_selected:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_selected:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED CLASSIFICATION REPORT - SELECTED FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test_cancer, y_pred_selected, \n",
    "                          target_names=['Malignant (0)', 'Benign (1)'],\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for selected features\n",
    "cm_selected = confusion_matrix(y_test_cancer, y_pred_selected)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_selected, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Malignant (0)', 'Benign (1)'],\n",
    "            yticklabels=['Malignant (0)', 'Benign (1)'],\n",
    "            annot_kws={'size': 14, 'weight': 'bold'})\n",
    "plt.title(f'Confusion Matrix - Selected Features ({n_features_to_select})', \n",
    "          fontsize=14, fontweight='bold', pad=15)\n",
    "plt.ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with Model Trained on All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with ALL features\n",
    "print(\"Training model with ALL features...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "lr_all = LogisticRegression(max_iter=10000, random_state=42)\n",
    "lr_all.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_all = lr_all.predict(X_test_cancer_scaled)\n",
    "y_pred_proba_all = lr_all.predict_proba(X_test_cancer_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics for all features\n",
    "acc_all = accuracy_score(y_test_cancer, y_pred_all)\n",
    "prec_all = precision_score(y_test_cancer, y_pred_all)\n",
    "rec_all = recall_score(y_test_cancer, y_pred_all)\n",
    "f1_all = f1_score(y_test_cancer, y_pred_all)\n",
    "roc_all = roc_auc_score(y_test_cancer, y_pred_proba_all)\n",
    "\n",
    "print(\"✓ Model trained successfully with all features!\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MODEL EVALUATION - ALL FEATURES ({X_train_cancer.shape[1]})\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy:  {acc_all:.4f}\")\n",
    "print(f\"Precision: {prec_all:.4f}\")\n",
    "print(f\"Recall:    {rec_all:.4f}\")\n",
    "print(f\"F1-Score:  {f1_all:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_all:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    f'Selected ({n_features_to_select})': [acc_selected, prec_selected, rec_selected, f1_selected, roc_selected],\n",
    "    f'All ({X_train_cancer.shape[1]})': [acc_all, prec_all, rec_all, f1_all, roc_all]\n",
    "})\n",
    "comparison['Difference'] = comparison[f'Selected ({n_features_to_select})'] - comparison[f'All ({X_train_cancer.shape[1]})']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PERFORMANCE COMPARISON: SELECTED vs ALL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\" * 80)\n",
    "if comparison['Difference'].mean() >= 0:\n",
    "    print(\"✓ Selected features perform BETTER or EQUAL to all features on average\")\n",
    "else:\n",
    "    print(\"⚠ All features perform slightly better than selected features\")\n",
    "    \n",
    "print(f\"\\nFeature Reduction: {X_train_cancer.shape[1]} → {n_features_to_select} features\")\n",
    "print(f\"Reduction Rate: {(1 - n_features_to_select/X_train_cancer.shape[1])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart comparison\n",
    "metrics = comparison['Metric'].tolist()\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, comparison[f'Selected ({n_features_to_select})'], \n",
    "                    width, label=f'Selected ({n_features_to_select})', \n",
    "                    color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = axes[0].bar(x + width/2, comparison[f'All ({X_train_cancer.shape[1]})'], \n",
    "                    width, label=f'All ({X_train_cancer.shape[1]})', \n",
    "                    color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "axes[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Difference chart\n",
    "colors_diff = ['green' if x >= 0 else 'red' for x in comparison['Difference']]\n",
    "bars = axes[1].bar(metrics, comparison['Difference'], color=colors_diff, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Difference (Selected - All)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Performance Difference', fontsize=14, fontweight='bold')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Experiment with Different Numbers of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different numbers of features\n",
    "print(\"Experimenting with different numbers of selected features...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_counts = [3, 5, 7, 10, 15, 20]\n",
    "results = []\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    print(f\"Testing with {n_features} features...\")\n",
    "    \n",
    "    # Create and fit RFE\n",
    "    rfe_temp = RFE(LogisticRegression(max_iter=10000, random_state=42), \n",
    "                   n_features_to_select=n_features)\n",
    "    rfe_temp.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "    \n",
    "    # Transform data\n",
    "    X_train_temp = X_train_cancer_scaled[:, rfe_temp.support_]\n",
    "    X_test_temp = X_test_cancer_scaled[:, rfe_temp.support_]\n",
    "    \n",
    "    # Train model\n",
    "    lr_temp = LogisticRegression(max_iter=10000, random_state=42)\n",
    "    lr_temp.fit(X_train_temp, y_train_cancer)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_temp = lr_temp.predict(X_test_temp)\n",
    "    y_pred_proba_temp = lr_temp.predict_proba(X_test_temp)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results.append({\n",
    "        'Features': n_features,\n",
    "        'Accuracy': accuracy_score(y_test_cancer, y_pred_temp),\n",
    "        'Precision': precision_score(y_test_cancer, y_pred_temp),\n",
    "        'Recall': recall_score(y_test_cancer, y_pred_temp),\n",
    "        'F1-Score': f1_score(y_test_cancer, y_pred_temp),\n",
    "        'ROC-AUC': roc_auc_score(y_test_cancer, y_pred_proba_temp)\n",
    "    })\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT RESULTS: DIFFERENT FEATURE COUNTS\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize experiment results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance vs Number of Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "colors_plot = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics_to_plot, colors_plot)):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    axes[row, col].plot(results_df['Features'], results_df[metric], \n",
    "                        marker='o', color=color, linewidth=2.5, markersize=10)\n",
    "    axes[row, col].set_xlabel('Number of Features', fontsize=11, fontweight='bold')\n",
    "    axes[row, col].set_ylabel(metric, fontsize=11, fontweight='bold')\n",
    "    axes[row, col].set_title(f'{metric} vs Features', fontsize=12, fontweight='bold')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    axes[row, col].set_xticks(results_df['Features'])\n",
    "\n",
    "# Hide the last subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of features\n",
    "optimal_idx = results_df['F1-Score'].idxmax()\n",
    "optimal_features = results_df.loc[optimal_idx]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DISCUSSION: IMPACT OF FEATURE SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. OPTIMAL FEATURE COUNT (Based on F1-Score):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Optimal number of features: {int(optimal_features['Features'])}\")\n",
    "print(f\"   F1-Score: {optimal_features['F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\n2. BENEFITS OF FEATURE SELECTION:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   ✓ Reduced model complexity\")\n",
    "print(\"   ✓ Faster training and prediction times\")\n",
    "print(\"   ✓ Improved model interpretability\")\n",
    "print(\"   ✓ Reduced risk of overfitting\")\n",
    "print(\"   ✓ Better generalization to new data\")\n",
    "\n",
    "print(\"\\n3. KEY OBSERVATIONS:\")\n",
    "print(\"-\" * 80)\n",
    "min_features = results_df.loc[results_df['Features'].idxmin()]\n",
    "max_features = results_df.loc[results_df['Features'].idxmax()]\n",
    "print(f\"   • With {int(min_features['Features'])} features: F1 = {min_features['F1-Score']:.4f}\")\n",
    "print(f\"   • With {int(max_features['Features'])} features: F1 = {max_features['F1-Score']:.4f}\")\n",
    "print(f\"   • Feature reduction of {(1 - n_features_to_select/X_train_cancer.shape[1])*100:.1f}% maintains strong performance\")\n",
    "\n",
    "print(\"\\n4. CONCLUSION:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   Feature selection using RFE effectively identifies the most important features\")\n",
    "print(\"   while maintaining or even improving model performance. The wrapper method\")\n",
    "print(\"   successfully balances model complexity and predictive accuracy.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Duitai ko summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"WORKSHEET 10 - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXERCISE 1: IMDB SENTIMENT ANALYSIS (NAIVE BAYES)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {len(df_imdb)} movie reviews\")\n",
    "print(f\"Model: Multinomial Naive Bayes with Bag-of-Words\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  • Accuracy:  {accuracy_nb:.4f} ({accuracy_nb*100:.2f}%)\")\n",
    "print(f\"  • Precision: {precision_nb:.4f}\")\n",
    "print(f\"  • Recall:    {recall_nb:.4f}\")\n",
    "print(f\"  • F1-Score:  {f1_nb:.4f}\")\n",
    "print(f\"  • ROC-AUC:   {roc_auc_nb:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXERCISE 3: BREAST CANCER FEATURE SELECTION (RFE)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {len(df_cancer)} samples with {X_cancer.shape[1]} features\")\n",
    "print(f\"Model: Logistic Regression with RFE\")\n",
    "print(f\"Feature Reduction: {X_cancer.shape[1]} → {n_features_to_select} features ({(1-n_features_to_select/X_cancer.shape[1])*100:.1f}% reduction)\")\n",
    "print(f\"\\nPerformance with {n_features_to_select} selected features:\")\n",
    "print(f\"  • Accuracy:  {acc_selected:.4f} ({acc_selected*100:.2f}%)\")\n",
    "print(f\"  • Precision: {prec_selected:.4f}\")\n",
    "print(f\"  • Recall:    {rec_selected:.4f}\")\n",
    "print(f\"  • F1-Score:  {f1_selected:.4f}\")\n",
    "print(f\"  • ROC-AUC:   {roc_selected:.4f}\")\n",
    "\n",
    "print(f\"\\nPerformance with all {X_cancer.shape[1]} features:\")\n",
    "print(f\"  • Accuracy:  {acc_all:.4f} ({acc_all*100:.2f}%)\")\n",
    "print(f\"  • F1-Score:  {f1_all:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY LEARNINGS\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Naive Bayes is effective for text classification tasks\")\n",
    "print(\"2. Proper text preprocessing improves model performance\")\n",
    "print(\"3. Feature selection (RFE) reduces complexity while maintaining accuracy\")\n",
    "print(\"4. Fewer features lead to more interpretable and efficient models\")\n",
    "print(\"5. Multiple evaluation metrics provide comprehensive performance insights\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
